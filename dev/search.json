[{"path":"/dev/LICENSE.html","id":null,"dir":"","previous_headings":"","what":"MIT License","title":"MIT License","text":"Copyright (c) 2022 fastadi authors Permission hereby granted, free charge, person obtaining copy software associated documentation files (“Software”), deal Software without restriction, including without limitation rights use, copy, modify, merge, publish, distribute, sublicense, /sell copies Software, permit persons Software furnished , subject following conditions: copyright notice permission notice shall included copies substantial portions Software. SOFTWARE PROVIDED “”, WITHOUT WARRANTY KIND, EXPRESS IMPLIED, INCLUDING LIMITED WARRANTIES MERCHANTABILITY, FITNESS PARTICULAR PURPOSE NONINFRINGEMENT. EVENT SHALL AUTHORS COPYRIGHT HOLDERS LIABLE CLAIM, DAMAGES LIABILITY, WHETHER ACTION CONTRACT, TORT OTHERWISE, ARISING , CONNECTION SOFTWARE USE DEALINGS SOFTWARE.","code":""},{"path":"/dev/authors.html","id":null,"dir":"","previous_headings":"","what":"Authors","title":"Authors and Citation","text":"Alex Hayes. Author, maintainer, copyright holder. Juhee Cho. Author. Donggyu Kim. Author. Karl Rohe. Author.","code":""},{"path":"/dev/authors.html","id":"citation","dir":"","previous_headings":"","what":"Citation","title":"Authors and Citation","text":"Hayes , Cho J, Kim D, Rohe K (2022). fastadi: Self-Tuning Data Adaptive Matrix Imputation. R package version 0.1.0.9000, https://github.com/RoheLab/fastadi.","code":"@Manual{,   title = {fastadi: Self-Tuning Data Adaptive Matrix Imputation},   author = {Alex Hayes and Juhee Cho and Donggyu Kim and Karl Rohe},   year = {2022},   note = {R package version 0.1.0.9000},   url = {https://github.com/RoheLab/fastadi}, }"},{"path":"/dev/index.html","id":"fastadi","dir":"","previous_headings":"","what":"Self-Tuning Data Adaptive Matrix Imputation","title":"Self-Tuning Data Adaptive Matrix Imputation","text":"fastadi implements AdaptiveImpute matrix completion algorithm. fastadi self-tuning alternative algorithms SoftImpute (implemented softImpute package), truncated SVD, maximum margin matrix factorization, weighted regularized matrix factorization (implemented rsparse package). simulations fastadi often outperforms softImpute small margin. may find fastadi useful developing embeddings sparsely observed data, working natural language processing, building recommendation system.","code":""},{"path":"/dev/index.html","id":"installation","dir":"","previous_headings":"","what":"Installation","title":"Self-Tuning Data Adaptive Matrix Imputation","text":"can install released version CRAN : can install development version GitHub :","code":"install.packages(\"fastadi\") # install.packages(\"devtools\") devtools::install_github(\"RoheLab/fastadi\")"},{"path":"/dev/index.html","id":"example-usage","dir":"","previous_headings":"","what":"Example usage","title":"Self-Tuning Data Adaptive Matrix Imputation","text":"embed users items MovieLens 100K dataset. Note vignettes currently scratch work reference developers yet ready general consumption.","code":"library(fastadi) #> Loading required package: LRMF3 #> Loading required package: Matrix mf <- adaptive_impute(ml100k, rank = 3L, max_iter = 5L) #> Warning:  #> Reached maximum allowed iterations. Returning early. mf #>  #> Adaptively Imputed Low Rank Matrix Factorization #> ------------------------------------------------ #>  #> Rank: 3 #>  #> Rows: 943 #> Cols: 1682 #>  #> d[rank]: 467.486 #> alpha:   144.663 #>  #> Components #>  #> u: 943 x 3 [matrix]  #> d: 3      [numeric]  #> v: 1682 x 3 [matrix]"},{"path":"/dev/index.html","id":"references","dir":"","previous_headings":"","what":"References","title":"Self-Tuning Data Adaptive Matrix Imputation","text":"Alex Hayes Karl Rohe. “Finding Topics Citation Data”. 2022+ Cho, Juhee, Donggyu Kim, Karl Rohe. “Asymptotic Theory Estimating Singular Vectors Values Partially-Observed Low Rank Matrix Noise.” Statistica Sinica, 2018. https://doi.org/10.5705/ss.202016.0205. ———. “Intelligent Initialization Adaptive Thresholding Iterative Matrix Completion: Statistical Algorithmic Theory Adaptive-Impute.” Journal Computational Graphical Statistics 28, . 2 (April 3, 2019): 323–33. https://doi.org/10.1080/10618600.2018.1518238. Mazumder, Rahul, Trevor Hastie, Robert Tibshirani. “Spectral Regularization Algorithms Learning Large Incomplete Matrices.” Journal Machine Learning Research, 2010. https://web.stanford.edu/~hastie/Papers/mazumder10a.pdf. can find original implementation accompanying papers .","code":""},{"path":"/dev/reference/adaptive_imputation.html","id":null,"dir":"Reference","previous_headings":"","what":"Create an Adaptive Imputation object — adaptive_imputation","title":"Create an Adaptive Imputation object — adaptive_imputation","text":"adaptive_imputation objects subclass LRMF3::svd_like(), additional field alpha.","code":""},{"path":"/dev/reference/adaptive_imputation.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Create an Adaptive Imputation object — adaptive_imputation","text":"","code":"adaptive_imputation(u, d, v, alpha, ...)"},{"path":"/dev/reference/adaptive_imputation.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Create an Adaptive Imputation object — adaptive_imputation","text":"u matrix \"left singular-ish\" vectors. d vector \"singular-ish\" values. v matrix \"right singular-ish\" vectors. alpha Value alpha final iteration. ... Optional additional items pass constructor.","code":""},{"path":"/dev/reference/adaptive_imputation.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Create an Adaptive Imputation object — adaptive_imputation","text":"adaptive_imputation object.","code":""},{"path":"/dev/reference/adaptive_impute.html","id":null,"dir":"Reference","previous_headings":"","what":"AdaptiveImpute — adaptive_impute","title":"AdaptiveImpute — adaptive_impute","text":"implementation AdaptiveImpute algorithm matrix completion sparse matrices.","code":""},{"path":"/dev/reference/adaptive_impute.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"AdaptiveImpute — adaptive_impute","text":"","code":"adaptive_impute(   X,   rank,   ...,   initialization = c(\"svd\", \"adaptive-initialize\", \"approximate\"),   max_iter = 200L,   check_interval = 1L,   epsilon = 1e-07,   additional = NULL )  # S3 method for sparseMatrix adaptive_impute(   X,   rank,   ...,   initialization = c(\"svd\", \"adaptive-initialize\", \"approximate\"),   additional = NULL )  # S3 method for LRMF adaptive_impute(   X,   rank,   ...,   epsilon = 1e-07,   max_iter = 200L,   check_interval = 1L )"},{"path":"/dev/reference/adaptive_impute.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"AdaptiveImpute — adaptive_impute","text":"X sparse matrix Matrix::sparseMatrix() class. rank Desired rank (integer) use low rank approximation. Must least 2L rank X. Note rank X typically unobserved computations may unstable even fail rank near exceeds threshold. ... Unused additional arguments. initialization initialize low rank approximation. Options : \"svd\" (default). initialization step, treats unobserved values zeroes. \"adaptive-initialize\". initialization step, treats unobserved values actually unobserved. However, current AdaptiveInitialize implementation relies dense matrix computations suitable relatively small matrices. \"approximate\". approximate variant AdaptiveInitialize less computationally expensive. See adaptive_initialize details. Note initialization matters AdaptiveImpute optimizes non-convex objective. current theory shows initializing AdaptiveInitialize leads consistent estimator, know case SVD initialization. Empirically found SVD initialization works well nonetheless. max_iter Maximum number iterations perform (integer). Defaults 200L. practice 10 iterations get decent approximation use exploratory analysis, 50-100 get way convergence. Must least 1L. check_interval Integer specifying often perform convergence checks. Defaults 1L. practice, check convergence requires norm calculation expensive large matrices decreasing frequency convergence checks reduce computation time. Can also set NULL, case max_iter iterations algorithm occur possibility stopping due small relative change imputed matrix. case delta reported Inf. epsilon Convergence criteria, measured terms relative change Frobenius norm full imputed matrix. Defaults 1e-7. additional Ignored except alpha_method = \"approximate\" case controls precise approximation alpha. approximate computation alpha always understand alpha, approximation better larger values additional. recommend making additional large computationally tolerable.","code":""},{"path":"/dev/reference/adaptive_impute.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"AdaptiveImpute — adaptive_impute","text":"low rank matrix factorization represented adaptive_imputation() object.","code":""},{"path":"/dev/reference/adaptive_impute.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"AdaptiveImpute — adaptive_impute","text":"Cho, Juhee, Donggyu Kim, Karl Rohe. “Asymptotic Theory Estimating Singular Vectors Values Partially-Observed Low Rank Matrix Noise.” Statistica Sinica, 2018. https://doi.org/10.5705/ss.202016.0205. ———. “Intelligent Initialization Adaptive Thresholding Iterative Matrix Completion: Statistical Algorithmic Theory Adaptive-Impute.” Journal Computational Graphical Statistics 28, . 2 (April 3, 2019): 323–33. https://doi.org/10.1080/10618600.2018.1518238.","code":""},{"path":"/dev/reference/adaptive_impute.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"AdaptiveImpute — adaptive_impute","text":"","code":"mf <- adaptive_impute(ml100k, rank = 3L, max_iter = 5L, check_interval = NULL) #> Warning:  #> Reached maximum allowed iterations. Returning early. mf #>  #> Adaptively Imputed Low Rank Matrix Factorization #> ------------------------------------------------ #>  #> Rank: 3 #>  #> Rows: 943 #> Cols: 1682 #>  #> d[rank]: 467.486 #> alpha:   144.663 #>  #> Components #>  #> u: 943 x 3 [matrix]  #> d: 3      [numeric]  #> v: 1682 x 3 [matrix]"},{"path":"/dev/reference/adaptive_initialize.html","id":null,"dir":"Reference","previous_headings":"","what":"AdaptiveInitialize — adaptive_initialize","title":"AdaptiveInitialize — adaptive_initialize","text":"implementation AdaptiveInitialize algorithm matrix imputation sparse matrices. moment implementation suitable small matrices order thousands rows columns .","code":""},{"path":"/dev/reference/adaptive_initialize.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"AdaptiveInitialize — adaptive_initialize","text":"","code":"adaptive_initialize(   X,   rank,   ...,   p_hat = NULL,   alpha_method = c(\"exact\", \"approximate\"),   additional = NULL )  # S3 method for sparseMatrix adaptive_initialize(   X,   rank,   ...,   p_hat = NULL,   alpha_method = c(\"exact\", \"approximate\"),   additional = NULL )"},{"path":"/dev/reference/adaptive_initialize.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"AdaptiveInitialize — adaptive_initialize","text":"X sparse matrix sparseMatrix class. Explicit (observed) zeroes X can dropped rank Desired rank (integer) use low rank approximation. Must least 2L rank X. ... Ignored. p_hat portion X observed. Defaults NULL, case p_hat set number observed elements X. Primarily internal use citation_impute() advanced users. alpha_method Either \"exact\" \"approximate\", defaulting \"exact\". \"exact\" computationally expensive requires taking complete SVD matrix size nrow(X) x nrow(X), matches AdaptiveInitialize algorithm exactly. \"approximate\" departs AdaptiveInitialization algorithm compute truncated SVD rank rank + additional instead complete SVD. reduces computational burden, resulting estimates singular-ish values penalized much AdaptiveInitialize algorithm. additional Ignored except alpha_method = \"approximate\" case controls precise approximation alpha. approximate computation alpha always understand alpha, approximation better larger values additional. recommend making additional large computationally tolerable.","code":""},{"path":"/dev/reference/adaptive_initialize.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"AdaptiveInitialize — adaptive_initialize","text":"low rank matrix factorization represented adaptive_imputation() object.","code":""},{"path":"/dev/reference/adaptive_initialize.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"AdaptiveInitialize — adaptive_initialize","text":"","code":"mf <- adaptive_initialize(   ml100k,   rank = 3,   alpha_method = \"approximate\",   additional = 2 )  mf #>  #> Adaptively Imputed Low Rank Matrix Factorization #> ------------------------------------------------ #>  #> Rank: 3 #>  #> Rows: 943 #> Cols: 1682 #>  #> d[rank]: 3359.134 #> alpha:   26.874 #>  #> Components #>  #> u: 943 x 3 [matrix]  #> d: 3      [numeric]  #> v: 1682 x 3 [matrix]"},{"path":"/dev/reference/citation_impute.html","id":null,"dir":"Reference","previous_headings":"","what":"CitationImpute — citation_impute","title":"CitationImpute — citation_impute","text":"implementation AdaptiveImpute algorithm using efficient sparse matrix computations, specialized case missing values upper triangle taken explicitly observed zeros, opposed missing values. primarily useful spectral decompositions adjacency matrices graphs (near) tree structure, citation networks.","code":""},{"path":"/dev/reference/citation_impute.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"CitationImpute — citation_impute","text":"","code":"citation_impute(   X,   rank,   ...,   initialization = c(\"svd\", \"adaptive-initialize\", \"approximate\"),   max_iter = 200L,   check_interval = 1L,   epsilon = 1e-07,   additional = NULL )  # S3 method for sparseMatrix citation_impute(   X,   rank,   ...,   initialization = c(\"svd\", \"adaptive-initialize\", \"approximate\"),   additional = NULL )  # S3 method for LRMF citation_impute(   X,   rank,   ...,   epsilon = 1e-07,   max_iter = 200L,   check_interval = 1L )"},{"path":"/dev/reference/citation_impute.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"CitationImpute — citation_impute","text":"X square sparse matrix Matrix::sparseMatrix() class. Implicit zeros upper triangle matrix considered observed predictions elements contribute objective function minimized AdaptiveImpute. rank Desired rank (integer) use low rank approximation. Must least 2L rank X. Note rank X typically unobserved computations may unstable even fail rank near exceeds threshold. ... Unused additional arguments. initialization initialize low rank approximation. Options : \"svd\" (default). initialization step, treats unobserved values zeroes. \"adaptive-initialize\". initialization step, treats unobserved values actually unobserved. However, current AdaptiveInitialize implementation relies dense matrix computations suitable relatively small matrices. \"approximate\". approximate variant AdaptiveInitialize less computationally expensive. See adaptive_initialize details. Note initialization matters AdaptiveImpute optimizes non-convex objective. current theory shows initializing AdaptiveInitialize leads consistent estimator, know case SVD initialization. Empirically found SVD initialization works well nonetheless. max_iter Maximum number iterations perform (integer). Defaults 200L. practice 10 iterations get decent approximation use exploratory analysis, 50-100 get way convergence. Must least 1L. check_interval Integer specifying often perform convergence checks. Defaults 1L. practice, check convergence requires norm calculation expensive large matrices decreasing frequency convergence checks reduce computation time. Can also set NULL, case max_iter iterations algorithm occur possibility stopping due small relative change imputed matrix. case delta reported Inf. epsilon Convergence criteria, measured terms relative change Frobenius norm full imputed matrix. Defaults 1e-7. additional Ignored except alpha_method = \"approximate\" case controls precise approximation alpha. approximate computation alpha always understand alpha, approximation better larger values additional. recommend making additional large computationally tolerable.","code":""},{"path":"/dev/reference/citation_impute.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"CitationImpute — citation_impute","text":"low rank matrix factorization represented adaptive_imputation() object.","code":""},{"path":"/dev/reference/citation_impute.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"CitationImpute — citation_impute","text":"OpenMP available, citation_impute automatically use getOption(\"Ncpus\", 1L) OpenMP threads parallelize key computations. Note computations performed Armadillo C++ linear algebra library may also parallelized dependent BLAS LAPACK installations configurations.","code":""},{"path":"/dev/reference/citation_impute.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"CitationImpute — citation_impute","text":"","code":"# create a (binary) square sparse matrix to demonstrate on  set.seed(887)  n <- 10 A <- rsparsematrix(n, n, 0.1, rand.x = NULL)  mf <- citation_impute(A, rank = 3L, max_iter = 1L, check_interval = NULL) #> Warning:  #> Reached maximum allowed iterations. Returning early. mf #>  #> Adaptively Imputed Low Rank Matrix Factorization #> ------------------------------------------------ #>  #> Rank: 3 #>  #> Rows: 10 #> Cols: 10 #>  #> d[rank]: 1.286 #> alpha:   0.345 #>  #> Components #>  #> u: 10 x 3 [matrix]  #> d: 3      [numeric]  #> v: 10 x 3 [matrix]"},{"path":"/dev/reference/fastadi-package.html","id":null,"dir":"Reference","previous_headings":"","what":"fastadi: Self-Tuning Data Adaptive Matrix Imputation — fastadi-package","title":"fastadi: Self-Tuning Data Adaptive Matrix Imputation — fastadi-package","text":"Implements AdaptiveImpute matrix completion algorithm 'Intelligent Initialization Adaptive Thresholding Iterative Matrix Completion', <https://amstat.tandfonline.com/doi/abs/10.1080/10618600.2018.1518238>. AdaptiveImpute useful embedding sparsely observed matrices, often performs competing matrix completion algorithms, self-tunes hyperparameter, making usage easy.","code":""},{"path":[]},{"path":"/dev/reference/fastadi-package.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"fastadi: Self-Tuning Data Adaptive Matrix Imputation — fastadi-package","text":"Maintainer: Alex Hayes alexpghayes@gmail.com (ORCID) [copyright holder] Authors: Juhee Cho Donggyu Kim Karl Rohe","code":""},{"path":"/dev/reference/masked_approximation_impl.html","id":null,"dir":"Reference","previous_headings":"","what":"Expand an SVD only at observed values of a sparse matrix — masked_approximation_impl","title":"Expand an SVD only at observed values of a sparse matrix — masked_approximation_impl","text":"TODO: describe looks like dimensions match s mask. See vignette(\"sparse-computations\") mathematical details.","code":""},{"path":"/dev/reference/masked_approximation_impl.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Expand an SVD only at observed values of a sparse matrix — masked_approximation_impl","text":"","code":"masked_approximation_impl(U, V, row, col)"},{"path":"/dev/reference/masked_approximation_impl.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Expand an SVD only at observed values of a sparse matrix — masked_approximation_impl","text":"U Low-rank matrix left singular-ish vectors. V Low-rank matrix right singular-ish vectors. row Zero-based row indices observed elements. col Zero-based col indices observed elements.","code":""},{"path":"/dev/reference/masked_approximation_impl.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Expand an SVD only at observed values of a sparse matrix — masked_approximation_impl","text":"sparse matrix representing low-rank reconstruction U, d V, index pairs indicated row col.","code":""},{"path":"/dev/reference/masked_approximation_impl.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Expand an SVD only at observed values of a sparse matrix — masked_approximation_impl","text":"idea populate U, d V using elements SVD-like list. can generate row col easily sparse masking Matrix (Matrix package), coercing triplet format, extracting mask@row mask@j column.","code":""},{"path":[]},{"path":"/dev/news/index.html","id":"fastadi-010","dir":"Changelog","previous_headings":"","what":"fastadi 0.1.0","title":"fastadi 0.1.0","text":"CRAN release: 2022-02-11 Added NEWS.md file track changes package.","code":""}]
