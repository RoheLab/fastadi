---
title: "Introduction to sparse computations"
author: "Alex Hayes"
date: "`r Sys.Date()`"
bibliography: references.bib
link-citations: true
urlcolor: blue
header-includes:
  \usepackage[linesnumbered,ruled,vlined]{algorithm2e}
  \DeclareMathOperator{\diag}{diag}
  \DeclareMathOperator{\trace}{trace}
  \DeclareMathOperator{\sign}{sign}
  \linespread{1.25}
  \usepackage{helvet}
  \renewcommand{\familydefault}{\sfdefault}
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Introduction to sparse computations}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r setup, include = FALSE}
knitr::opts_chunk$set(echo = TRUE, eval = FALSE)
```

\newcommand \bv {\mathbf{v}}
\newcommand \bu {\mathbf{u}}
\newcommand \bl {\boldsymbol{\lambda}}

## Abstract

TODO: motivate ugh this is a show

This paper is a tutorial covering the computational details associated with computing the low-rank rank adaptive imputations for matrices described in [@cho_asymptotic_2015; @cho_intelligent_2018]. This work extends previous data adaptive matrix imputation strategies such as that of [@mazumder_spectral_2010], and has better performance while eliminating tuning parameters.

The tutorial proceeds in three parts. First, we introduce the imputation algorithms, useful tidbits of linear algebra and the R package `Matrix`, which we will use to illustrate computations. After a naive initial implementation which eats up lots of memory, we demonstrate a memory-efficient implementation. Finally, we extend this memory-efficient implementation to the partially observed matrices that possibly have a large number of observed zeros.

[@bates_introduction_2005; @cho_asymptotic_2015; @cho_intelligent_2018; @maechler_2nd_2006; @mazumder_spectral_2010; @bro_resolving_2007]


## Notation & Algorithm

There are two steps to computing the low rank approximation of [@cho_intelligent_2018]. First we use compute an initial low rank estimate with the `AdaptiveInitialize` algorithm. This step is essentially a debiased SVD. We then use the initial solution as a seed for the `AdaptiveImpute` algorithm, a form of iterative SVD thresholding with a data adaptive thresholding parameter.

The input to these algorithms is a partially observed matrix $M$, and $r$, the desired rank of the low-rank approximation. We define $Y$ to be an indicator matrix that tells us whether or not we observed a particular value of $M$.

\begin{algorithm}
\linespread{1.6}\selectfont
\DontPrintSemicolon
\KwIn{$M, Y$ and $r$}
$\hat p \gets \frac{1}{nd} \sum_{i=1}^n \sum_{j=1}^d Y_{ij}$ \;
$\Sigma_{\hat p} \gets M^T M - (1 - \hat p) \diag(M^T M)$ \;
$\Sigma_{t \hat p} \gets M M^T - (1 - \hat p) \diag(M M^T)$ \;
$\hat V_i \gets \bv_i(\Sigma_{\hat p})$ for $i = 1, ..., r$ \;
$\hat U_i \gets \bu_i(\Sigma_{t \hat p})$ for $i = 1, ..., r$ \;
$\tilde \alpha \gets \frac{1}{d - r} \sum_{i=r+1}^d \bl_i (\Sigma_{\hat p})$ \;
$\hat \lambda_i \gets \frac{1}{\hat p} \sqrt{\bl_i (\Sigma_{\hat p}) - \tilde \alpha}$ for $i = 1, ..., r$ \;
$\hat s_i \gets \sign(\langle \hat V_i, \bv_i (M) \rangle) \sign(\langle \hat U_i, \bu_i (M) \rangle)$ for $i = 1, ..., r$ \;
$\hat \lambda_i \gets \hat s_i \cdot \hat \lambda_i$ \;
\Return{$\hat \lambda_i, \hat U_i, \hat V_i$ for $i = 1, ..., r$}\;
\caption{\texttt{AdaptiveInitialize}}
\end{algorithm}

Here $\bu_i, \bl_i, \bv_i$ are functions that return the $i^{th}$ left singular vector, singular value, and right singular value, respectively. $\tilde \alpha$ is the data adaptive thresholding parameter. Note that it is the average of the uncalculated singular values, and is thus positive.

In line 8, $\langle , \rangle$ denotes the Frobenius inner norm. That is, for vectors $x, y$ and matrices $A, B$:

\begin{align}
\langle x, y \rangle &= x^T y = \sum_{i} x_i y_i \\
\langle A, B \rangle &= \sum_{i, j} A_{ij} B_{ij} = \sum_{ij} A \odot B \\
\end{align}

We use $A \odot B$ to mean the elementwise (Hadamard) product of matrices $A$ and $B$.

We can use the output of `AdaptiveInitialize` to construct a low rank approximation to $M$ via:

\begin{align}
\hat M = \sum_{i=1}^r \hat \lambda_i \hat U_i \hat V_i^T
\end{align}

Next up is `AdaptiveImpute`:

\begin{algorithm}
\linespread{1.6}\selectfont
\DontPrintSemicolon
\KwIn{$M, Y, r$ and $\varepsilon > 0$}

$Z^{(1)} \gets \texttt{AdaptiveInitialize}(M, Y, r)$ \;

\Repeat{$\| Z_{t+1} - Z_t \|^2_F / \| Z_{t+1} \|_F$}{
  
  $\tilde M^{(t)} \gets P_\Omega (M) + P_\Omega^\perp (Z_t)$ \;
  $\hat V_i^{(t)} \gets \bv_i(\tilde M^{(t)})$ for $i = 1, ..., r$ \;
  $\hat U_i^{(t)} \gets \bu_i(\tilde M^{(t)})$ for $i = 1, ..., r$ \;
  $\tilde \alpha^{(t)} \gets \frac{1}{d - r} \sum_{i=r+1}^d \bl_i^2 (\tilde M^{(t)})$ \;
  $\hat \lambda_i^{(t)} \gets \sqrt{\bl_i^2 (\tilde M^{(t)}) - \tilde \alpha^{(t)}}$ for $i = 1, ..., r$ \;
  $Z^{(t+1)} \gets \sum_{i=1}^r \hat \lambda_i^{(t)} \hat U_i^{(t)} \hat V_i^{(t)^T}$ \;
  $t \gets t + 1$ \;
}
\Return{$\hat \lambda_i^{(t)}, \hat U_i^{(t)}, \hat V_i^{(t)}$ for $i = 1, ..., r$}\;
\caption{\texttt{AdaptiveImpute}}
\end{algorithm}

Here we again have some new notation. First we define

\begin{align}
P_\Omega(A) &= A \odot Y \\
P_\Omega^\perp (A) &= A \odot (1 - Y)
\end{align}

These are the projecttion of a matrix $A$ onto the observed of $M$ and the unobserved elements of $M$, respectively. Similar $\Omega$ is the set of all pairs $(i, j)$ such that $M_{i, j}$ is observed. $\bl_i^2 (A)$ is a function that returns the $i^{th}$ *squared* singular value of $A$ (i.e. $\bl_i^2 (A) = \left(\bl_i (A)\right)^2$). Finally $\Vert \cdot \Vert_F$ is the Frobenius norm of a matrix.

## Pre-requisites

### The Matrix package

If you haven't used the `Matrix` package before, we recommend reading [this introduction][matrix_intro] as well as this [2nd introduction][matrix_2nd_intro].

[matrix_intro]: https://cran.r-project.org/web/packages/Matrix/vignettes/Intro2Matrix.pdf
[matrix_2nd_intro]: https://cran.r-project.org/web/packages/Matrix/vignettes/Intro2Matrix.pdf

```{r}
library(Matrix)

set.seed(17)

# create a random 8 x 12 sparse matrix with 30 nonzero entries
M <- rsparsematrix(8, 12, nnz = 30)
M
```


```{r}
summary(M)
```

TODO: different storage formats for sparse matrices. CSC, triplet, symmetric. for the most part `Matrix` does the right thing for you. The triplet form will be most important for us later on when we right out some matrix multiplications by hand.

```{r}
# note that Matrix objects are S4 classes so we access their
# slots using the @ symbol
class(M)

M@x  # vector of values in M
M@i  # corresponding row indices

# if you want column indices you need a dgTMatrix
M2 <- as(M, "dgTMatrix")
M2@j  # corresponding column indices // only for dgCMatrix object
```

We will repeatedly calculate squared Frobenious norms throughout this tutorial. It's important to know that there are *many, many* ways to calculate this norm. We will almost always calculate the squared Frobenius norm of a sparse Matrix `M` via `sum(M@x^2)`. That said, these are all equivalent:

```{r}
M^2    # square each element in M elementwise, return as sparse matrix
M@x^2  # square each element in M elementwise, return as vector of nonzeros

# the second version is much faster
bench::mark(
  sum(M@x^2),
  sum(M^2),
  sum(colSums(M^2)),
  norm(M, type = "F")^2,
  sum(M * M),
  iterations = 20
)
```

The projections $P_\Omega (A)$ and $P_\Omega^\perp (A)$ where $\Omega$ indicates the observed elements of a matrix $M$ and $A$ is another matrix with the same dimensions as $M$.

```{r}
y <- as(M, "lgCMatrix")  # indicator matrix only
all.equal(y * M, M)  # don't lose anything multiplying by indicators

A <- matrix(1:(8*12), 8, 12)

all.equal(dim(A), dim(M))  # appropriate to practice projections with

# Omega indicates whether an entry of M was observed

# P_Omega (A)
A * y

!y

# P_Omega^perp (A): NOTE: this results in a *dense* matrix
A * (1 - y)

all(A * y + A * (1 - y) == A)  # can recover A from both projections together
```

crossproducts

```{r}
bench::mark(
  crossprod(M),     # dsCMatrix -- most specialized class, want this
  crossprod(M, M),  # dgCMatrix
  t(M) %*% M,       # dgCMatrix
  check = FALSE
)
```

the `drop()` function helps us manage dimensions

```{r}
one_col <- matrix(1:4)
one_row <- matrix(5:8, nrow = 1)

drop(one_col)
drop(one_row)

c(one_col)  # same thing, less explicit. use drop to be explicit
```

diagonal of crossproduct

```{r eval = FALSE}
v_sign == colSums(svd_M$v * v_hat)
diag(t(svd_M$v) %*% v_hat)
diag(crossprod(svd_M$v, v_hat))

bench::mark(
  colSums(svd_M$v * v_hat),
  crossprod(rep(1, d), svd_M$v * v_hat),
  iterations = 50,
  check = FALSE
)

# write a diag_crossprod helper
```

```{r}
rhos <- matrix(1:12, ncol = 4, byrow = TRUE)

bench::mark(
  diag(crossprod(rhos)),
  diag(t(rhos) %*% rhos),
  colSums(rhos * rhos),
  crossprod(rep(1, nrow(rhos)), rhos^2),
  check = FALSE                            
)


rhos <- matrix(1:12, ncol = 4, byrow = TRUE)

bench::mark(
  diag(tcrossprod(rhos)),
  diag(crossprod(t(rhos))),
  diag(rhos %*% t(rhos)),
  rowSums(rhos * rhos),
  check = FALSE
)
```

what we get from `eigen()` and `svd()`: slightly different stuff: `u`, `d` and `v` versus `values` and `vectors`

NOTE: RSpectra *only* does truncated decompositions. if you want the full decomposition, you have to use base R stuff. different algos.

## Brief aside in sign ambiguity

yada yada yada the signs of the left and right singular vectors are not identified in SVD

A more elegant solution as proposed in @bro_resolving_2007 and used in Karl's paper is to take inner products

NOTE TO SELF: identifying the signs of a single SVD is a much harder task than comparing two SVDs and seeing if they are the same up to sign differences. we only need to check if they are the same up to sign differences.

```{r}
set.seed(17)
M <- rsparsematrix(8, 12, nnz = 30) # small example, not very sparse

# number of singular vectors to compute
k <- 4

s <- svd(M, k, k)
s2 <- svds(M, k, k)

# irritating: svd() always gives you all the singular values even if you 
# only request the first K singular vectors
s$u %*% diag(s$d[1:k]) %*% t(s$v)

# based on the flip_signs function of
# https://stats.stackexchange.com/questions/134282/relationship-between-svd-and-pca-how-to-use-svd-to-perform-pca
equal_svds <- function(s, s2) {
  
  # svd() always gives you all the singular values, but we only
  # want to compare the first k
  k <- ncol(s$u)
  
  # the term sign(s$u) * sign(s2$u) performs a sign correction
  
  # isTRUE because output of all.equal is not a boolean, it's something
  # weird when the inputs aren't equal. lol why
  
  u_ok <- isTRUE(
    all.equal(s$u, s2$u * sign(s$u) * sign(s2$u), check.attributes = FALSE)
  )
  
  v_ok <- isTRUE(
    all.equal(s$v, s2$v * sign(s$v) * sign(s2$v), check.attributes = FALSE)
  )
  
  d_ok <- isTRUE(all.equal(s$d[1:k], s2$d[1:k], check.attributes = FALSE))
  
  u_ok && v_ok && d_ok
}
```

### Linear algebra facts

Throughout these computations, we will repeatedly use several key facts about eigendecompositions, singular value decompositions (SVD) and the relationship between the two.

https://en.wikipedia.org/wiki/Gramian_matrix X'X -- positive semi-def, so the singular values of M'M are the same as the eigenvalues

question: if A, B positive, is
sum(svd(A - B)$d) == sum(svd(A)$d) - sum(svd(B)$d)

answer: NO! can't split into two easy computations and then combine them
possibly use this to get some sort of bound?

think about what happens as p_hat -> 0

A key observation here is that $M^T M$ and

Fact: sum of squared singular values is `trace(A^T A)`
https://math.stackexchange.com/questions/2281721/sum-of-singular-values-of-a-matrix

Fact: for symmetric positive definite matrices the eigendecomp is equal to the singular value decomp

Fact: sum of eigenvalues of M is equal to trace(M)

Consequence: for pos def symmetric M the sum of the singular values is trace(M) as well

Again computing `alpha` deserves some explanation.

- reference: https://math.stackexchange.com/questions/1463269/how-to-obtain-sum-of-square-of-eigenvalues-without-finding-eigenvalues
- Frobenius norm (A) = trace(crossprod(A))
- TODO: how we know this thing is strictly positive to prevent sqrt() from exploding

```{r}
## STOPPED HERE: WHY ARE the following not the same?
  isSymmetric(sigma_p)
  eigen(sigma_p)$values
  sum(diag(sigma_p))
  sum(svd(sigma_p)$d)
  sum(eigen(sigma_p)$values)
  
  # let's think just about the first term for a moment
  sum(diag(MtM / p_hat^2))
  sum(svd(MtM / p_hat^2)$d)
  
  sum(colSums(M^2 / p_hat^2))
  
  # has some negative eigenvalues
  
  # Fact: for a symmetric matrix, the singular values are the *absolute* values
  # of the eigenvalues
  
  # https://www.mathworks.com/content/dam/mathworks/mathworks-dot-com/moler/eigs.pdf
  
  eigen(sigma_p)$values
  sum(eigen(sigma_p)$values)
  sum(abs(eigen(sigma_p)$values))
  sum(svd(sigma_p)$d)
  sum(abs(diag(sigma_p)))
  
  # those agree so what about the second term
  
  # note to self: alpha should be positive
  # issue karl ran into:
  # https://math.stackexchange.com/questions/381808/sum-of-eigenvalues-and-singular-values
  # how to get the sum of singular values itself (start):
  # https://math.stackexchange.com/questions/569989/sum-of-singular-values-of-ab
  
  # options when sigma_p is not positive definite:
  # - calculate the full SVD
  # - set alpha to zero (don't truncate the singular values)
  # - this lower bounds the average of the remaining singular values
  # -
  
  # positive semi-definite is enough since symmetric and eigen/singular values
  # of zero don't matter
  
  # this is only an issue in the initialization. in the iterative updates
  # we use the squared singular values, which we can more easily calculate
  # the sum of
  
  # ask Karl what he wants to do about this: computing a full SVD is gonna be really expensive.
```

FACT: sum(diag(crossprod(M))) == sum(M^2)

## Reference implementation

```{r}
library(RSpectra)
library(Matrix)
```

<!-- ```{#AdaptiveImpute .R .numberLines} -->

```{r}
adaptive_initialize <- function(M, r) {
  
  # TODO: ignores observed zeros!
  p_hat <- nnzero(M) / prod(dim(M))  # line 1
  
  MtM <- crossprod(M)
  MMt <- tcrossprod(M)
  
  # need to divide by p^2 from Cho et al 2016 to get the "right"
  # singular values / singular values on a comparable scale
  
  # both of these matrices are symmetric, but not necessarily positive
  # this has important implications for the SVD / eigendecomp relationship
  
  sigma_p <- MtM / p_hat^2 - (1 - p_hat) * diag(diag(MtM))  # line 2
  sigma_t <- MMt / p_hat^2 - (1 - p_hat) * diag(diag(MMt))  # line 3
  
  # crossprod() and tcrossprod() return dsCMatrix objects,
  # sparse matrix objects that know they are symmetric
  
  # unfortunately, RSpectra doesn't support dsCMatrix objects,
  # but does support dgCMatrix objects, a class representing sparse
  # but not symmetric matrices
  
  # support for dsCMatrix objects in RSpectra is on the way,
  # which will eliminate the need for the following coercions.
  # see: https://github.com/yixuan/RSpectra/issues/15
  
  sigma_p <- as(sigma_p, "dgCMatrix")
  sigma_t <- as(sigma_t, "dgCMatrix")
  
  svd_p <- svds(sigma_p, r)  # TODO: is eigs_sym() faster?
  svd_t <- svds(sigma_t, r)
  
  v_hat <- svd_p$v  # line 4
  u_hat <- svd_t$u  # line 5
  
  n <- nrow(M)
  d <- ncol(M)
  
  # NOTE: alpha is incorrect due to singular values and eigenvalues
  # being different when sigma_p is not positive
  
  alpha <- (sum(diag(sigma_p)) - sum(svd_p$d)) / (d - r)  # line 6
  lambda_hat <- sqrt(svd_p$d - alpha) / p_hat             # line 7
  
  svd_M <- svds(M, r)
  
  v_sign <- crossprod(rep(1, d), svd_M$v * v_hat)
  u_sign <- crossprod(rep(1, n), svd_M$u * u_hat)
  s_hat <- drop(sign(v_sign * u_sign))
  
  lambda_hat <- lambda_hat * s_hat  # line 8
  
  list(u = u_hat, d = lambda_hat, v = v_hat)
}
```


It's worth commenting on computation of `alpha` and `s_hat`.

When we compute `alpha` in line 20 `adaptive_initialize()`, we don't want to do the full eigendecomposition of $\Sigma_{\hat p}$ since that could take a long time, so we use trick and recall that the trace of a matrix (the sum of it's diagonal elements) equals the sum of all the eigenvalues. Then we subtract off the first $r$ eigenvalues, which we do compute, and are left with $\sum_{i = r + 1}^d \lambda_i(\Sigma_{\hat p})$.

<!-- ```{#AdaptiveImpute .R .numberLines} -->
```{r}
adaptive_impute <- function(M, r, epsilon = 1e-7) {
  
  s <- adaptive_initialize(M, r)
  Z <- s$u %*% diag(s$d) %*% t(s$v)  # line 1
  delta <- Inf
  
  while (delta > epsilon) {
    
    y <- as(M, "lgCMatrix")  # indicator if entry of M observed
    M_tilde <- M + Z * (1 - y)  # line 3
    
    svd_M <- svds(M_tilde, r)
    
    u_hat <- svd_M$u  # line 4
    v_hat <- svd_M$v  # line 5
    
    d <- ncol(M)
    
    alpha <- (sum(M_tilde^2) - sum(svd_M$d^2)) / (d - r)  # line 6
    
    lambda_hat <- sqrt(svd_M$d^2 - alpha)  # line 7
    
    Z_new <- u_hat %*% diag(lambda_hat) %*% t(v_hat)
    
    delta <- sum((Z_new - Z)^2) / sum(Z^2)
    Z <- Z_new
    
    print(glue::glue("delta: {round(delta, 8)}, alpha: {round(alpha, 3)}"))
  }
  
  Z
}
```

Finally we can do a minimal sanity check and see if this code even runs, and see if we are recovering something close-ish to implanted low-rank structure.

```{r eval = FALSE}
n <- 500
d <- 100
r <- 5

A <- matrix(runif(n * r, -5, 5), n, r)
B <- matrix(runif(d * r, -5, 5), d, r)
M0 <- A %*% t(B)

err <- matrix(rnorm(n * d), n, d)
Mf <- M0 + err

p <- 0.3
y <- matrix(rbinom(n * d, 1, p), n, d)
dat <- Mf * y

init <- adaptive_initialize(dat, r)
filled <- adaptive_impute(dat, r)
```

## References
