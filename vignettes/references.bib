
@article{cho_intelligent_2018,
	title = {Intelligent {Initialization} and {Adaptive} {Thresholding} for {Iterative} {Matrix} {Completion}; {Some} {Statistical} and {Algorithmic} {Theory} for {Adaptive}-{Impute}},
	issn = {1061-8600, 1537-2715},
	url = {https://www.tandfonline.com/doi/full/10.1080/10618600.2018.1518238},
	doi = {10.1080/10618600.2018.1518238},
	abstract = {Over the past decade, various matrix completion algorithms have been developed. Thresholded singular value decomposition (SVD) is a popular technique in implementing many of them. A sizable number of studies have shown its theoretical and empirical excellence, but choosing the right threshold level still remains as a key empirical diﬃculty. This paper proposes a novel matrix completion algorithm which iterates thresholded SVD with theoretically-justiﬁed and data-dependent values of thresholding parameters. The estimate of the proposed algorithm enjoys the ∗This research is supported by NSF grant DMS-1309998 and ARO grant W911NF-15-1-0423.},
	language = {en},
	urldate = {2018-12-10},
	journal = {Journal of Computational and Graphical Statistics},
	author = {Cho, Juhee and Kim, Donggyu and Rohe, Karl},
	month = sep,
	year = {2018},
	pages = {1--26},
	file = {Cho et al. - 2018 - Intelligent Initialization and Adaptive Thresholdi.pdf:C\:\\Users\\alex\\Zotero\\storage\\3936LKKS\\Cho et al. - 2018 - Intelligent Initialization and Adaptive Thresholdi.pdf:application/pdf}
}

@article{maechler_2nd_2006,
	title = {2nd {Introduction} to the {Matrix} package},
	url = {https://cran.r-project.org/web/packages/Matrix/vignettes/Intro2Matrix.pdf},
	abstract = {Linear algebra is at the core of many areas of statistical computing and from its inception the S language has supported numerical linear algebra via a matrix data type and several functions and operators, such as \%*\%, qr, chol, and solve. However, these data types and functions do not provide direct access to all of the facilities for eﬃcient manipulation of dense matrices, as provided by the Lapack subroutines, and they do not provide for manipulation of sparse matrices.},
	urldate = {2019-05-15},
	author = {Maechler, Martin and Bates, Douglas},
	year = {2006},
	file = {Maechler and Bates - 2nd Introduction to the Matrix package.pdf:C\:\\Users\\alex\\Zotero\\storage\\C5GQGHLI\\Maechler and Bates - 2nd Introduction to the Matrix package.pdf:application/pdf}
}

@article{bates_introduction_2005,
	title = {Introduction to the {Matrix} package},
	url = {https://cran.r-project.org/web/packages/Matrix/vignettes/Introduction.pdf},
	abstract = {Linear algebra is at the core of many areas of statistical computing and from its inception the S language has supported numerical linear algebra via a matrix data type and several functions and operators, such as \%*\%, qr, chol, and solve. However, these data types and functions do not provide direct access to all of the facilities for eﬃcient manipulation of dense matrices, as provided by the Lapack subroutines, and they do not provide for manipulation of sparse matrices.},
	language = {en},
	urldate = {2019-05-15},
	author = {Bates, Douglas},
	year = {2005},
	file = {Bates - 2005 - Introduction to the Matrix package.pdf:C\:\\Users\\alex\\Zotero\\storage\\TTSG6YLB\\Bates - 2005 - Introduction to the Matrix package.pdf:application/pdf}
}

@article{mazumder_spectral_2010,
	title = {Spectral {Regularization} {Algorithms} for {Learning} {Large} {Incomplete} {Matrices}},
	url = {https://web.stanford.edu/~hastie/Papers/mazumder10a.pdf},
	abstract = {We use convex relaxation techniques to provide a sequence of regularized low-rank solutions for large-scale matrix completion problems. Using the nuclear norm as a regularizer, we provide a simple and very efﬁcient convex algorithm for minimizing the reconstruction error subject to a bound on the nuclear norm. Our algorithm SOFT-IMPUTE iteratively replaces the missing elements with those obtained from a soft-thresholded SVD. With warm starts this allows us to efﬁciently compute an entire regularization path of solutions on a grid of values of the regularization parameter. The computationally intensive part of our algorithm is in computing a low-rank SVD of a dense matrix. Exploiting the problem structure, we show that the task can be performed with a complexity of order linear in the matrix dimensions. Our semideﬁnite-programming algorithm is readily scalable to large matrices; for example SOFT-IMPUTE takes a few hours to compute low-rank approximations of a 106 × 106 incomplete matrix with 107 observed entries, and ﬁts a rank-95 approximation to the full Netﬂix training set in 3.3 hours. Our methods achieve good training and test errors and exhibit superior timings when compared to other competitive state-of-the-art techniques.},
	language = {en},
	author = {Mazumder, Rahul and Hastie, Trevor and Tibshirani, Robert},
	year = {2010},
	file = {Mazumder et al. - 2010 - Spectral Regularization Algorithms for Learning La.pdf:C\:\\Users\\alex\\Zotero\\storage\\QMXUWKEE\\Mazumder et al. - 2010 - Spectral Regularization Algorithms for Learning La.pdf:application/pdf}
}

@article{bro_resolving_2007,
	title = {Resolving the {Sign} {Ambiguity} in the {Singular} {Value} {Decomposition}},
	abstract = {Many modem data analysis methods involve computing a matrix singular value decomposition (SVD) or eigenvalue decomposition (EVD). Principal components analysis is the time-honored example, but more recent applications include latent semantic indexing, hypertext induced topic selection (HITS), clustering, classification, etc. Though the SVD and EVD are well-established and can be computed via state-of-the-art algorithms, it is not commonly mentioned that there is an intrinsic sign indeterminacy that can significantly impact the conclusions and interpretations drawn from their results. Here we provide a solution to the sign ambiguity problem and show how it leads to more sensible solutions.},
	language = {en},
	author = {Bro, Rasmus and Acar, Evrim and Kolda, Tamara G.},
	year = {2007},
	pages = {18},
	file = {Bro et al. - 2007 - Resolving the Sign Ambiguity in the Singular Value.pdf:C\:\\Users\\alex\\Zotero\\storage\\97QLQJL3\\Bro et al. - 2007 - Resolving the Sign Ambiguity in the Singular Value.pdf:application/pdf}
}

@article{cho_asymptotic_2015,
	title = {Asymptotic {Theory} for {Estimating} the {Singular} {Vectors} and {Values} of a {Partially}-observed {Low} {Rank} {Matrix} with {Noise}},
	url = {http://arxiv.org/abs/1508.05431},
	abstract = {Matrix completion algorithms recover a low rank matrix from a small fraction of the entries, each entry contaminated with additive errors. In practice, the singular vectors and singular values of the low rank matrix play a pivotal role for statistical analyses and inferences. This paper proposes estimators of these quantities and studies their asymptotic behavior. Under the setting where the dimensions of the matrix increase to inﬁnity and the probability of observing each entry is identical, Theorem 1 gives the rate of convergence for the estimated singular vectors; Theorem 3 gives a multivariate central limit theorem for the estimated singular values. Even though the estimators use only a partially observed matrix, they achieve the same rates of convergence as the fully observed case. These estimators combine to form a consistent estimator of the full low rank matrix that is computed with a non-iterative algorithm. In the cases studied in this paper, this estimator achieves the minimax lower bound in Koltchinskii et al. (2011a). The numerical experiments corroborate our theoretical results.},
	language = {en},
	urldate = {2019-05-10},
	journal = {arXiv:1508.05431 [stat]},
	author = {Cho, Juhee and Kim, Donggyu and Rohe, Karl},
	month = aug,
	year = {2015},
	note = {arXiv: 1508.05431},
	keywords = {Statistics - Methodology},
	file = {Cho et al. - 2015 - Asymptotic Theory for Estimating the Singular Vect.pdf:C\:\\Users\\alex\\Zotero\\storage\\YCBQPX7P\\Cho et al. - 2015 - Asymptotic Theory for Estimating the Singular Vect.pdf:application/pdf}
}