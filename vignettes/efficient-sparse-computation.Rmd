---
title: "Using memory-efficient sparse computations"
author: "Alex Hayes"
date: "`r Sys.Date()`"
output: pdf_document
bibliography: references.bib
link-citations: true
urlcolor: blue
header-includes:
  \usepackage[linesnumbered,ruled,vlined]{algorithm2e}
  \DeclareMathOperator{\diag}{diag}
  \DeclareMathOperator{\trace}{trace}
  \DeclareMathOperator{\sign}{sign}
  \linespread{1.25}
  \usepackage{helvet}
  \renewcommand{\familydefault}{\sfdefault}
vignette: >
  %\VignetteIndexEntry{Using memory-efficient sparse computations}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>",
  message = FALSE,
  warning = FALSE,
  eval = FALSE
)
```

\newcommand \bv {\mathbf{v}}
\newcommand \bu {\mathbf{u}}
\newcommand \bl {\boldsymbol{\lambda}}

## Low-rank implementation

The reference implementation has some problems. As our data matrix $M$ gets larger, we can no longer fit the dense representation of $\hat M$ and $Z^{(t)}$ into memory. Instead, we need to work with just the low rank components $\hat \lambda, \hat U$ and $\hat V$.

This leads us to following implementation:

```{r}
low_rank_adaptive_initialize <- function(M, r) {
  
  M <- as(M, "dgCMatrix")
  
  p_hat <- nnzero(M) / prod(dim(M))  # line 1
  
  # NOTE: skip explicit computation of line 2
  # NOTE: skip explicit computation of line 3
  
  eig_p <- eigen_helper(M, r)
  eig_t <- eigen_helper(t(M), r)
  
  lr_v_hat <- eig_p$vectors  # line 4
  lr_u_hat <- eig_t$vectors  # line 5
  
  d <- ncol(M)
  n <- nrow(M)
  
  # NOTE: alpha is again incorrect since we work with eigenvalues
  # rather than singular values here
  sum_eigen_values <- sum(M@x^2) / p^2 - (1 - p) * sum(colSums(M^2))
  lr_alpha <- (sum_eigen_values - sum(eig_p$values)) / (d - r)  # line 6
  
  lr_lambda_hat <- sqrt(eig_p$values - lr_alpha) / p_hat  # line 7
  
  # TODO: Karl had another sign computation here that he said was faster
  # but it wasn't documented anywhere, so I'm going with what was in the 
  # paper
  
  lr_svd_M <- svds(M, r)
  
  # v_hat is d by r
  lr_v_sign <- crossprod(rep(1, d), lr_svd_M$v * lr_v_hat)
  lr_u_sign <- crossprod(rep(1, n), lr_svd_M$u * lr_u_hat)
  lr_s_hat <- c(sign(lr_v_sign * lr_u_sign))  # line 8
  
  lr_lambda_hat <- lr_lambda_hat * lr_s_hat
  
  list(u = lr_u_hat, d = lr_lambda_hat, v = lr_v_hat)
}
```

What does `eigen_helper()` do? Describe the return object (also do this for svds)

```{r}
# Take the eigendecomposition of t(M) %*% M - (1 - p) * diag(t(M) %*% M)
# using sparse computations only
eigen_helper <- function(M, r) {
  eigs_sym(
    Mx, r,
    n = ncol(M),
    args = list(
      M = M,
      p = nnzero(M) / prod(dim(M))
    )
  )
}

# compute (t(M) %*% M / p^2 - (1 - p) * diag(diag(t(M) %*% M))) %*% x
# using sparse operations

# TODO: divide the second term by p^2 like in the reference implementatio
Mx <- function(x, args) {
  drop(
    crossprod(args$M, args$M %*% x) / args$p^2 - (1 - args$p) * Diagonal(ncol(args$M), colSums(args$M^2)) %*% x
  )
}
```

Now we check that `Mx` works

```{r}
x <- rnorm(12)
p <- 0.3
out <- (t(M) %*% M / p^2 - (1 - p) * diag(diag(t(M) %*% M))) %*% x
out2 <- Mx(x, args = list(M = M, p = p))
all.equal(as.matrix(out), as.matrix(out))
```

# check the computation of alpha for p.s.d. matrices

```{r}
S <- toeplitz((10:1)/10)
set.seed(11)
R <- rWishart(1, 20, S)[, ,1]
R

sum(diag(R))
sum(eigen(R)$values)
sum(svd(R)$d)

p <- 0.5

X <- crossprod(R) - (1 - p) * diag(crossprod(R))


sum(diag(X))
sum(eigen(X)$values)
sum(svd(X)$d)
```


```{r}
M <- ml100k
s <- svd(M)
```

```{r}
# nuclear norm
sum(s$d)
norm(M, type = "F")

r <- 5

d <- ncol(M)
n <- nrow(M)

alpha <- (sum(s$d) - sum(s$d[1:r])) / (d - r)
alpha

s$d[1:r]
```



TODO: update alg description to divide by $p^2$ to get the right singular values

Quickly check that the components works before we try the code that integrates them all together



Finally, sanity check this by comparing to the reference implementation. These don't agree, which isn't great:

```{r}
lr_init <- sparse_adaptive_initialize(dat, r)

# some weird stuff is happening with the singular values but I'm
# going to not worry about it for the time being

equal_svds(init, lr_init)
```

## Space-efficient adaptive impute

TODO: figure out the actual space complexity

Recall the algorithm looks like

\begin{align}
\hat M = \sum_{i=1}^r \hat s_i  \hat \lambda_i \hat U_i \hat V_i^T
\end{align}

\begin{algorithm}
\linespread{1.6}\selectfont
\DontPrintSemicolon
\KwIn{$M, y, r$ and $\varepsilon > 0$}

$Z^{(1)} \gets \texttt{AdaptiveInitialize}(M, y, r)$ \;

\Repeat{$\| Z_{t+1} - Z_t \|^2_F / \| Z_{t+1} \|_F$}{
  
  $\tilde M^{(t)} \gets P_\Omega (M) + P_\Omega^\perp (Z_t)$ \;
  $\hat V_i^{(t)} \gets \bv_i(\tilde M^{(t)})$ for $i = 1, ..., r$ \;
  $\hat U_i^{(t)} \gets \bu_i(\tilde M^{(t)})$ for $i = 1, ..., r$ \;
  $\tilde \alpha^{(t)} \gets \frac{1}{d - r} \sum_{i=r+1}^d \bl_i^2 (\tilde M^{(t)})$ \;
  $\hat \lambda_i^{(t)} \gets \sqrt{\bl_i^2 (\tilde M^{(t)}) - \tilde \alpha^{(t)}}$ for $i = 1, ..., r$ \;
  $Z^{(t+1)} \gets \sum_{i=1}^r \hat \lambda_i^{(t)} \hat U_i^{(t)} \hat V_i^{(t)^T}$ \;
  $t \gets t + 1$ \;
}
\Return{$\hat \lambda_i^{(t)}, \hat U_i^{(t)}, \hat V_i^{(t)}$ for $i = 1, ..., r$}\;
\caption{\texttt{AdapativeImpute}}
\end{algorithm}

Now we need two things:

1. The SVD of $\tilde M^{(t)}$
2. (Certain sums of) the squared singular values.

### Sums of squared singular values

For a matrix $A$, the sum of squared singular values (denoted by $\lambda_i$) equals the squared frobenius norm:

\begin{align}
\sum_{i=1}^{\min(n, d)} \lambda_i^2 = ||A||_F^2 = \trace(A^T A)
\end{align}

Also note that 

\begin{align}
||A + B||_F^2 = ||A||_F^2 + ||B||_F^2 + 2 \cdot \langle A, B \rangle_F
\end{align}

Now we consider $\tilde M^{(t)}$. Suppose that unobserved values of $M$ are set to zero, as is the case for $M$ stored in a sparse matrix representation

\begin{align}
\tilde M^{(t)} &= P_\Omega(M) + P_\Omega^\perp (Z_t) \\
&= P_\Omega(M) + P_\Omega^\perp \left(
  \sum_{i=1}^r \hat \lambda_i^{(t)} \hat U_i^{(t)} \hat V_i^{(t)^T}
  \right)
\end{align}

Now we need

\begin{align}
||\tilde M^{(t)}||_F^2 
&= \left \Vert
  P_\Omega(M) + P_\Omega^\perp \left(
    \sum_{i=1}^r \hat \lambda_i^{(t)} \hat U_i^{(t)} \hat V_i^{(t)^T}
    \right)
  \right \Vert_F^2 \\
&= \left \Vert P_\Omega(M) \right \Vert_F^2 
  + \left \Vert P_\Omega^\perp \left(
      \sum_{i=1}^r \hat \lambda_i^{(t)} \hat U_i^{(t)} \hat V_i^{(t)^T}
    \right)
  \right \Vert_F^2
  + 2 \cdot \left \langle P_\Omega(M), P_\Omega^\perp \left(
      \sum_{i=1}^r \hat \lambda_i^{(t)} \hat U_i^{(t)} \hat V_i^{(t)^T}
    \right) \right \rangle_F \\
&= \left \Vert P_\Omega(M) \right \Vert_F^2 
  + \left \Vert P_\Omega^\perp \left(
      \sum_{i=1}^r \hat \lambda_i^{(t)} \hat U_i^{(t)} \hat V_i^{(t)^T}
    \right)
  \right \Vert_F^2
\end{align}

Where the cancellation in the final line follows because

\begin{align}
\left \langle P_\Omega(M), P_\Omega^\perp
  \left(
    \sum_{i=1}^r \hat \lambda_i^{(t)} \hat U_i^{(t)} \hat V_i^{(t)^T}
  \right)
\right \rangle_F
= \sum_{i, j} P_\Omega(M)_{ij} \cdot P_\Omega^\perp (Z_t)_{ij}
= \sum_{i, j} 0
= 0
\end{align}

Now we need one more trick, which is that

\begin{align}
\left \Vert Z_t \right \Vert_F^2
= \left \Vert P_\Omega (Z_t) + P_\Omega^\perp (Z_t) \right \Vert_F^2
= \left \Vert P_\Omega (Z_t) \right \Vert_F^2 + 
  \left \Vert P_\Omega^\perp (Z_t) \right \Vert_F^2
\end{align}

and then

\begin{align}
\left \Vert P_\Omega^\perp (Z_t) \right \Vert_F^2
= \left \Vert Z_t \right \Vert_F^2
  - \left \Vert P_\Omega (Z_t) \right \Vert_F^2 \\
= \sum_{i = 1}^r \lambda_i^2 - \left \Vert Z_t \odot Y \right \Vert_F^2
\end{align}

Putting it all together we see

\begin{align}
||\tilde M^{(t)}||_F^2 
&= \left \Vert P_\Omega(M) \right \Vert_F^2 
  + \left \Vert P_\Omega^\perp \left(
      \sum_{i=1}^r \hat \lambda_i^{(t)} \hat U_i^{(t)} \hat V_i^{(t)^T}
    \right)
  \right \Vert_F^2 \\
&= \left \Vert M \right \Vert_F^2 + 
  \sum_{i = 1}^r \lambda_i^2 - 
  \left \Vert Z_t \odot Y \right \Vert_F^2
\end{align}

In code we will have a sparse matrix `M` and a list `s` with elements of the SVD. The first Frobenious norm is quick to calculate, but I am not sure how to calculate the other two frobenius norms.

```{r}
# s is a matrix defined in terms of it's svd
# G is a sparse matrix
# compute only elements of U %*% diag(d) %*% t(V) only on non-zero elements of G
# G and U %*% t(V) must have same dimensions

# maybe call this svd_perp?
svd_perp <- function(s, mask) {
  
  # note: must be dgTMatrix to get column indexes j larger
  # what if we used dlTMatrix here?
  m <- as(mask, "dgTMatrix")
  
  # the indices for which we want to compute the matrix multiplication
  # turn zero based indices into one based indices
  i <- m@i + 1
  j <- m@j + 1

  # gets rows and columns of U and V to multiply, then multiply
  ud <- s$u %*% diag(s$d)
  left <- ud[i, ]
  right <- s$v[j, ]

  # compute inner products to get elements of U %*% t(V)
  uv <- rowSums(left * right)

  # NOTE: specify dimensions just in case
  sparseMatrix(i = i, j = j, x = uv, dims = dim(mask))
}
```

Test it

```{r}
set.seed(17)

M <- rsparsematrix(8, 12, nnz = 30)
s <- svds(M, 5)

y <- as(M, "lgCMatrix")

Z <- s$u %*% diag(s$d) %*% t(s$v)

all.equal(
  svd_perp(s, M),
  Z * y
)
```

So, to take an eigendecomp you just need to be able to do $Mx$. To take an SVD, what do you need? matrix vector and matrix transpose vector multiplication

```{r}
set.seed(17)
r <- 5

M <- rsparsematrix(8, 12, nnz = 30)
y <- as(M, "lgCMatrix")

s <- svds(M, r)
Z <- s$u %*% diag(s$d) %*% t(s$v)

M_tilde <- M + Z * (1 - y)  # dense!

Z_perp <- svd_perp(s, M)
sum_singular_squared <- sum(M@x^2) + sum(s$d^2) - sum(Z_perp@x^2)

all.equal(
  sum(svd(M_tilde)$d^2),
  sum_singular_squared
)
```

### SVD of M tilde

```{r}
set.seed(17)
r <- 5

M <- rsparsematrix(8, 12, nnz = 30)
y <- as(M, "lgCMatrix")

s <- svds(M, r)
Z <- s$u %*% diag(s$d) %*% t(s$v)

M_tilde <- M + Z * (1 - y)  # dense!

svd_M_tilde <- svds(M_tilde, r)
svd_M_tilde
```


```{r}
Ax <- function(x, args) {
  drop(M_tilde %*% x)
}

Atx <- function(x, args) {
  drop(t(M_tilde) %*% x)
}

# is eigs_sym() with a two-sided multiply faster?
args <- list(u = s$u, d = s$d, v = s$v, m = M)
test1 <- svds(Ax, k = r, Atrans = Atx, dim = dim(M), args = args)

test1
svd_M_tilde

all.equal(
  svd_M_tilde,
  test1
)
```

So we're done our first sanity check of the function interface. Let $x$ be a vector. Now we want to calculate

\begin{align}
\tilde M^{(t)} x 
  &= \left[ P_\Omega(M) + P_\Omega^\perp (Z_t) \right] x \\
  &= \left[ P_\Omega(M) -
    P_\Omega (Z_t) +
    P_\Omega (Z_t) +
    P_\Omega^\perp (Z_t) \right] x \\
  &= P_\Omega(M - Z_t) x + Z_t x
\end{align}

where we can think of $R_t \equiv P_\Omega(M - Z_t)$ as "residuals" of sorts. Crucially, $R_t$ is sparse, and

\begin{align}
Z_t x &= (\hat U 
  \diag(\hat \lambda_1, ..., \hat \lambda_r)
  \hat V^t) x \\
  &= (\hat U 
  (\diag(\hat \lambda_1, ..., \hat \lambda_r)
  (\hat V^t x)))
\end{align}

So now the memory requirement of the computation has been reduced to that of two sparse matrix vector multiplications, rather than that of fitting the dense matrix $P_\Omega^\perp (Z_t)$ into memory.

Similarly, for the transpose, we have

\begin{align}
\tilde M^{{(t)}^T} x 
  &= \left[ P_\Omega(M) + P_\Omega^\perp (Z_t) \right]^T x \\
  &= \left[ P_\Omega(M) -
    P_\Omega (Z_t) +
    P_\Omega (Z_t) +
    P_\Omega^\perp (Z_t) \right]^T x \\
  &= P_\Omega(M - Z_t)^T x + Z_t^T x
\end{align}

This leads us to a second, less memory intensive implementation of `Ax()` and `Atx()`:

```{r}
# input: M, Z_t as a low-rank SVD list s

R <- M - svd_perp(s, M)  # residual matrix
args <- list(u = s$u, d = s$d, v = s$v, R = R)

Ax <- function(x, args) {
  drop(args$R %*% x + args$u %*% diag(args$d) %*% crossprod(args$v, x))
}

Atx <- function(x, args) {
  # TODO: can we use a crossprod() for the first multiplication here?
  drop(t(args$R) %*% x + args$v %*% diag(args$d) %*% crossprod(args$u, x))
}

# is eigs_sym() with a two-sided multiply faster?
test2 <- svds(Ax, k = r, Atrans = Atx, dim = dim(M), args = args)

all.equal(
  svd_M_tilde,
  test2
)
```

```{r}
relative_f_norm_change <- function(s_new, s) {
  # TODO: don't do the dense calculation here
  
  Z_new <- s_new$u %*% diag(s_new$d) %*% t(s_new$v)
  Z_new <- s$u %*% diag(s$d) %*% t(s$v)
  
  sum((Z_new - Z)^2) / sum(Z^2)
}

```


```{r}
sparse_adaptive_impute <- function(M, r, epsilon = 1e-03) {

  # coerce M to sparse matrix such that we use sparse operations
  M <- as(M, "dgCMatrix")
  
  # low rank svd-like object, s ~ Z_1
  s <- sparse_adaptive_initialize(M, r)  # line 1
  delta <- Inf
  d <- ncol(M)
  norm_M <- sum(M@x^2)

  while (delta > epsilon) {
    
    # update s: lines 4 and 5
    # take the SVD of M-tilde
    
    R <- M - svd_perp(s, M)  # residual matrix
    args <- list(u = s$u, d = s$d, v = s$v, R = R)
    
    s_new <- svds(Ax, k = r, Atrans = Atx, dim = dim(M), args = args)
    
    MtM <- norm_M + sum(s_new$d^2) - sum(svd_perp(s_new, M)^2)
    alpha <- (sum(MtM) - sum(s_new$d^2)) / (d - r)  # line 6
    
    s_new$d <- sqrt(s_new$d^2 - alpha)  # line 7
    
    # NOTE: skip explicit computation of line 8
    delta <- relative_f_norm_change(s_new, s)
    
    s <- s_new
    
    print(glue::glue("delta: {round(delta, 8)}, alpha: {round(alpha, 3)}"))
  }
  
  s
}
```

```{r}
out <- sparse_adaptive_impute(M, r)
out
```


\begin{align}
\texttt{MtM} = \tilde M^{{(t)}^T} \tilde M^{(t)}
\end{align}


## Extension to a mixture of observed and unobserved missingness

originally solving an optimization vaguely of the form

\begin{align}
\left \Vert M - \hat M \right \Vert_F^2
\end{align}

where $M$ is a partially observed matrix. now, we let $M' = Y M$ where $Y$ is an indicator of whether or not $M$ was observed. Typically we have some setup like

\begin{align}
M = 
 \begin{bmatrix}
    \cdot & \cdot & 3     & 1     & \cdot \\
    3     & \cdot & \cdot & 8     & \cdot \\
    \cdot & -1    & \cdot & \cdot & \cdot \\
    \cdot & \cdot & \cdot & \cdot & \cdot \\
    \cdot & 2     & \cdot & \cdot & \cdot \\
    5     & \cdot & 7     & \cdot & 4
  \end{bmatrix}
, \qquad
Y = 
  \begin{bmatrix}
    \cdot & \cdot & 1     & 1     & \cdot \\
    1     & \cdot & \cdot & 1     & \cdot \\
    \cdot & 1     & \cdot & \cdot & \cdot \\
    \cdot & \cdot & \cdot & \cdot & \cdot \\
    \cdot & 1     & \cdot & \cdot & \cdot \\
    1     & \cdot & 1     & \cdot & 1
  \end{bmatrix}
\end{align}

Here the symbol $\cdot$ means that an entry of the matrix was unobserved.

but what we do now is, continuing to represent $M$ as a sparse matrix with no zero entries, is *observe* a bunch of zeros. So we might know that the upper triangule of $M$ has structurally missing zeros that we have observed are missing. These zeros are primarily important because they affect the residuals in our calculations. In this particular case, the take multiplication by $M$, a sparse operation, and make it into a dense operation.

At this point it becomes useful to introduce some additional notation. Let $\tilde \Omega$ be the set of indicies $(i, j)$ such that $M_{i, j}$ is non-zero. Observe that $\tilde \Omega \subset \Omega$. Then we have $P_{\tilde \Omega} (A) = P_\Omega (A)$.

\begin{align}
M = 
 \begin{bmatrix}
    0     & 0     & 3     & 1     & 0 \\
    3     & 0     & 0     & 8     & 0 \\
    \cdot & -1    & 0     & 0     & 0 \\
    \cdot & \cdot & \cdot & 0     & 0 \\
    \cdot & 2     & \cdot & \cdot & 0 \\
    5     & \cdot & 7     & \cdot & 4
  \end{bmatrix}
, \qquad
Y = 
  \begin{bmatrix}
    1     & 1     & 1     & 1     & 1 \\
    1     & 1     & 1     & 1     & 1 \\
    \cdot & 1     & 1     & 1     & 1 \\
    \cdot & \cdot & \cdot & 1     & 1 \\
    \cdot & 1     & \cdot & \cdot & 1 \\
    1     & \cdot & 1     & \cdot & 1
  \end{bmatrix}
, \qquad
M' = 
 \begin{bmatrix}
    \cdot & \cdot & 3     & 1     & \cdot \\
    3     & \cdot & \cdot & 8     & \cdot \\
    \cdot & -1    & \cdot & \cdot & \cdot \\
    \cdot & \cdot & \cdot & \cdot & \cdot \\
    \cdot & 2     & \cdot & \cdot & \cdot \\
    5     & \cdot & 7     & \cdot & 4
  \end{bmatrix}
\end{align}

Temporary scratch for presentation


\begin{align}
A = 
 \begin{bmatrix}
    0 & 0 & 1 & 1 & 0 \\
    ? & 0 & 0 & 1 & 0 \\
    ? & ? & 0 & 0 & 0 \\
    ? & ? & ? & 0 & 1 \\
    ? & ? & ? & ? & 0 \\
    ? & ? & ? & ? & 0
  \end{bmatrix}
\end{align}

And now we need to figure out how to calculate

\begin{align}
\tilde M^{(t)} x 
  &= \left[ P_\Omega(M) + P_\Omega^\perp (Z_t) \right] x \\
  &= \left[ P_\Omega(M) -
    P_\Omega (Z_t) +
    P_\Omega (Z_t) +
    P_\Omega^\perp (Z_t) \right] x \\
  &= P_\Omega(M)  - P_\Omega(Z_t) x + Z_t x \\
  &= P_{\tilde \Omega}(M)  - P_\Omega(Z_t) x + Z_t x
\end{align}

We already know how to calculate $P_{\tilde \Omega}(M)$ and $Z_t x$ using sparse operations, so we're left with $P_\Omega(Z_t) x$. Note that $P_\Omega(Z_t) x \neq P_{\tilde \Omega} (Z_t) x$ since $Z_t$ is not necessarily zero on $\tilde \Omega^\perp$. In other words $P_\Omega^\perp (Z_t) \neq Z_t$.

When $Y$ is dense, there is no way to avoid paying the computational cost of the dense or near dense computation. Our primary concern is fitting $Y$ into memory for large datasets. This is an issue when $Y$ is dense but with no discernible structure that permits a more compact representation.

If we can fit $Y$ into memory, we can do a low-rank computation, only calculating elements $Z^{(t)}_{ij}$ when $Y_{ij} = 1$. When $Y$ is stored as a vector of row indices together with a vector of column indices (plus some information about the dimension), we can write the computation out:

```{r}
M <- Matrix(
  rbind(
    c(0, 0, 3, 1, 0),
    c(3, 0, 0, 8, 0),
    c(0, -1, 0, 0, 0),
    c(0, 0, 0, 0, 0),
    c(0, 2, 0, 0, 0),
    c(5, 0, 7, 0, 4)
  )
)

Y <- rbind(
  c(1, 1, 1, 1, 1),
  c(1, 1, 1, 1, 1),
  c(0, 1, 1, 1, 1),
  c(0, 0, 1, 1, 1),
  c(0, 1, 0, 1, 1),
  c(1, 0, 1, 0, 1)
)

s <- svds(M, 2)

Y <- as(Y, "CsparseMatrix")

# triplet form
# compressed column matrix form even better but don't
# understand the format
Y <- as(Y, "lgCMatrix")
Y <- as(Y, "lgTMatrix")

# ugh: RcppArmadillo only supports dgTMatrix rather than
# lgTMatrix which is somewhat unfortunate

# link: https://cran.r-project.org/web/packages/RcppArmadillo/vignettes/RcppArmadillo-sparseMatrix.pdf

Y

x <- rnorm(5)

# want to calculate
Z <- s$u %*% diag(s$d) %*% t(s$v)
out <- drop((Z * Y) %*% x)
out
```

At this point it's worth writing out explicitly how calculate $Z^{(t)}_{ij}$.

\begin{align}
Z^{(t)}_{ij} 
&= \left( \sum_{\ell=1}^r \hat U_\ell \hat d_\ell \hat V_\ell^T \right)_{ij}
\end{align}

For a visual reminder, this looks like (using $\diag(\hat d)$ and $\hat \Sigma$ somewhat interchangeably here)

\begin{align}
Z^{(t)}
= \hat U \diag(\hat d) \, \hat V^T
= 
\begin{bmatrix}
  U_1 & U_2 & \hdots & U_r \\
\end{bmatrix}
\begin{bmatrix}
  d_{1} & 0 & \hdots & 0 \\
  0 & d_{2} & \hdots & 0 \\
  \vdots & \vdots & \ddots & \vdots \\
  0 & 0 & \hdots & d_{r}
\end{bmatrix}
\begin{bmatrix}
  V_1^T \\
  V_2^T \\
  \vdots \\
  V_r^T
\end{bmatrix}
\end{align}

\begin{align}
\begin{bmatrix}
  U_{11} & U_{12} & \hdots & U_{1r} \\
  U_{21} & U_{22} & \hdots & U_{2r} \\
  \vdots & \vdots & \ddots & \vdots \\
  U_{n1} & U_{n2} & \hdots & U_{nr}
\end{bmatrix}
\begin{bmatrix}
  d_{1} & 0 & \hdots & 0 \\
  0 & d_{2} & \hdots & 0 \\
  \vdots & \vdots & \ddots & \vdots \\
  0 & 0 & \hdots & d_{r}
\end{bmatrix}
\begin{bmatrix}
  V_{11} & V_{21} & \hdots & V_{d1} \\
  V_{12} & V_{22} & \hdots & V_{d2} \\
  \vdots & \vdots & \ddots & \vdots \\
  V_{1r} & V_{2r} & \hdots & V_{dr}
\end{bmatrix}
\end{align}

n x d = (n x r) x (r x r) x (r x d)

(r x d) is after the transpose

\begin{align}
\begin{bmatrix}
  U_{11} & U_{12} & \hdots & U_{1r} \\
  U_{21} & U_{22} & \hdots & U_{2r} \\
  \vdots & \vdots & \ddots & \vdots \\
  U_{n1} & U_{n2} & \hdots & U_{nr}
\end{bmatrix}
\begin{bmatrix}
  d_{1} & 0 & \hdots & 0 \\
  0 & d_{2} & \hdots & 0 \\
  \vdots & \vdots & \ddots & \vdots \\
  0 & 0 & \hdots & d_{r}
\end{bmatrix}
\begin{bmatrix}
  V_{11} & V_{21} & \hdots & V_{d1} \\
  V_{12} & V_{22} & \hdots & V_{d2} \\
  \vdots & \vdots & \ddots & \vdots \\
  V_{1r} & V_{2r} & \hdots & V_{dr}
\end{bmatrix}
\end{align}

Also because I never remember anything, recall that $Z$ is $n \times d$, $x$ is $d \times 1$ and $Zx$ is $n \times 1$:

\begin{align}
(Zx)_i = \sum_{j=1}^d Z_{ij} \cdot x_j
\end{align}

(recall that $X_i$ always refers to the $i^{th}$ column of $X$ in this document)

```{r}
# mask as a pair list
# L and Z / svd are both n x d matrices
# x is a d x 1 matrix / vector
masked_svd_times_x <- function(s, mask, x) {
  
  stopifnot(inherits(mask, "lgTMatrix"))
  
  u <- s$u
  d <- s$d
  v <- s$v
  
  zx <- numeric(nrow(u))
  
  # lgTMatrix uses zero based indexing, add one
  row <- mask@i + 1
  col <- mask@j + 1
  
  # need to loop over index of indexes
  # double looping over i and j here feels intuitive
  # but is incorrect
  for (idx in seq_along(row)) {
    i <- row[idx]
    j <- col[idx]
    
    z_ij <- sum(u[i, ] * d * v[j, ])
    zx[i] <- zx[i] + x[j] * z_ij
  }
  
  zx
}

# how to calculate just one element of the reconstructed
# data using the SVD

i <- 6
j <- 4

sum(s$u[i, ] * s$d * s$v[j, ])
Z[i, j]

# the whole masked matrix multiply

Z <- s$u %*% diag(s$d) %*% t(s$v)
out <- drop((Z * Y) %*% x)

# check that we did this right
all.equal(
  masked_svd_times_x(s, Y, x),
  out
)
```

This is gonna be painfully slow in R so we rewrite in C++

```{Rcpp}
#include <RcppArmadillo.h>

using namespace arma;

// [[Rcpp::depends(RcppArmadillo)]]
// [[Rcpp::export]]

vec masked_svd_times_x_impl(
  const mat& U,
  const rowvec& d,
  const mat& V,
  const vec& row,
  const vec& col,
  const vec& x) {
  
  int i, j;
  double z_ij;
  
  vec zx = zeros<vec>(U.n_rows);
  
  for (int idx = 0; idx < row.n_elem; idx++) {
    
    i = row(idx);
    j = col(idx);
    
    // % does elementwise multiplication in Armadillo
    // accu() gives the sum of elements of resulting vector
    z_ij = accu(U.row(i) % d % V.row(j));
    
    zx(i) += x(j) * z_ij;
  }
  
  return zx;
}
```

```{r}
# wrap with slightly nicer interface
masked_svd_times_x_cpp <- function(s, mask, x) {
  drop(masked_svd_times_x_impl(s$u, s$d, s$v, mask@i, mask@j, x))
}
```

```{r}
bench::mark(
  masked_svd_times_x_cpp(s, Y, x),
  masked_svd_times_x(s, Y, x)
)
```

TODO:

- the tranpose multiplication
- the average singular value calculation

Recall that to calculate the average singular value we want

\begin{align}
||\tilde M^{(t)}||_F^2
&= \left \Vert M \right \Vert_F^2 + 
  \sum_{i = 1}^r \lambda_i^2 - 
  \left \Vert Z_t \odot Y \right \Vert_F^2
\end{align}

This is the other computationally intensive part so let's write it up in Armadillo as well

```{Rcpp}
#include <RcppArmadillo.h>

using namespace arma;

// [[Rcpp::depends(RcppArmadillo)]]
// [[Rcpp::export]]

double p_omega_f_norm_impl(
      const mat& U,
      const rowvec& d,
      const mat& V,
      const vec& row,
      const vec& col) {
  
    int i, j;
    double total = 0;
  
    for (int idx = 0; idx < row.n_elem; idx++) {
  
      i = row(idx);
      j = col(idx);
  
      total += accu(U.row(i) % d % V.row(j));
    }
  
    return total;
  }
```

```{r}
# wrap with slightly nicer interface
p_omega_f_norm_cpp <- function(s, mask) {
  p_omega_f_norm_impl(s$u, s$d, s$v, mask@i, mask@j)
}
```

```{r}
all.equal(
  sum(Z * Y),
  p_omega_f_norm_cpp(s, Y)
)
```


### A naive solution: the epsilon trick

issue: we've moved back into dense computation land

### The memory efficient version

asdf


TODO: don't coerce to explicit Matrix class as that is not recommended apparently. alternatives?

## Fully C++ implementation: Dense

```{Rcpp}
#include <RcppArmadillo.h>

using namespace arma;

// [[Rcpp::depends(RcppArmadillo)]]
// [[Rcpp::export]]
Rcpp::List AdaptiveInitialize(const sp_mat& M, const int r) {

  // coerce to double to avoid integer division
  double p_hat = static_cast<double>(M.n_nonzero) / (M.n_cols * M.n_rows);
  
  sp_mat MtM = M.t() * M;
  sp_mat MMt = M * M.t();
  
  sp_mat sigma_p = MtM / pow(p_hat, 2) - (1 - p_hat) * diagmat(MtM);
  sp_mat sigma_t = MMt / pow(p_hat, 2) - (1 - p_hat) * diagmat(MMt);

  mat U_p, V_p, U_t, V_t;
  vec s_p, s_t;

  svds(U_p, s_p, V_p, sigma_p, r);
  svds(U_t, s_t, V_t, sigma_t, r);

  // TODO: this is still the eigenvalue calculation
  double alpha = (sum(sigma_p.diag()) - sum(s_p)) / (M.n_cols - r);

  vec lambda_hat = sqrt(s_p - alpha) / p_hat;

  mat U_m, V_m;
  vec s_m;

  svds(U_m, s_m, V_m, M, r);

  // sum(A % B) finds the diag(A^T B) / the diagonal of the cross product
  // sum() does a *column-wise sum*, % an elementwise multiplication
  rowvec u_sign = sign(sum(U_m % U_t));
  rowvec v_sign = sign(sum(V_m % V_p));

  rowvec s_hat = u_sign % v_sign;
  lambda_hat = lambda_hat % conv_to< vec >::from(s_hat);

  return Rcpp::List::create(Rcpp::Named("u") = U_t,
                            Rcpp::Named("d") = lambda_hat,
                            Rcpp::Named("v") = V_p);
}
```

```{Rcpp}
#include <RcppArmadillo.h>

using namespace arma;

// [[Rcpp::depends(RcppArmadillo)]]
// [[Rcpp::export]]
Rcpp::List AdaptiveImpute(const sp_mat& M, const int r) {
  
  const double EPSILON = 0.000001;
  
  
  
  s <- adaptive_initialize(M, r)
  Z <- s$u %*% diag(s$d) %*% t(s$v)  # line 1
  delta <- Inf
  
  while (delta > epsilon) {
    
    y <- as(M, "lgCMatrix")  # indicator if entry of M observed
    M_tilde <- M + Z * (1 - y)  # line 3
    
    svd_M <- svds(M_tilde, r)
    
    u_hat <- svd_M$u  # line 4
    v_hat <- svd_M$v  # line 5
    
    d <- ncol(M)
    
    alpha <- (sum(M_tilde^2) - sum(svd_M$d^2)) / (d - r)  # line 6
    
    lambda_hat <- sqrt(svd_M$d^2 - alpha)  # line 7
    
    Z_new <- u_hat %*% diag(lambda_hat) %*% t(v_hat)
    
    delta <- sum((Z_new - Z)^2) / sum(Z^2)
    Z <- Z_new
    
    print(glue::glue("delta: {round(delta, 8)}, alpha: {round(alpha, 3)}"))
  }
  
  Z
}
```
