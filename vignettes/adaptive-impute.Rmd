---
title: "An introduction to AdaptiveInitialize and AdaptiveImpute"
author: "Alex Hayes"
date: "`r Sys.Date()`"
output: 
  pdf_document:
    extra_dependencies:
      algorithm2e: ["linesnumbered", "ruled", "vlined"]
bibliography: references.bib
link-citations: true
urlcolor: blue
vignette: >
  %\VignetteIndexEntry{An introduction to AdaptiveInitialize and AdaptiveImpute}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r setup, include = FALSE}
knitr::opts_chunk$set(echo = TRUE, eval = FALSE)
```

\newcommand \diag {\mathrm{diag}}
\newcommand \trace {\mathrm{trace}}
\newcommand \sign {\mathrm{sign}}

\newcommand \bv {\mathbf{v}}
\newcommand \bu {\mathbf{u}}
\newcommand \bl {\boldsymbol{\lambda}}

## Abstract

TODO: motivate ugh this is a show

This paper is a tutorial covering the computational details associated with computing the low-rank rank adaptive imputations for matrices described in [@cho_asymptotic_2015; @cho_intelligent_2018]. This work extends previous data adaptive matrix imputation strategies such as that of [@mazumder_spectral_2010], and has better performance while eliminating tuning parameters.

The tutorial proceeds in three parts. First, we introduce the imputation algorithms, useful tidbits of linear algebra and the R package `Matrix`, which we will use to illustrate computations. After a naive initial implementation which eats up lots of memory, we demonstrate a memory-efficient implementation. Finally, we extend this memory-efficient implementation to the partially observed matrices that possibly have a large number of observed zeros.

[@bates_introduction_2005; @cho_asymptotic_2015; @cho_intelligent_2018; @maechler_2nd_2006; @mazumder_spectral_2010; @bro_resolving_2007]


## Notation & Algorithm

There are two steps to computing the low rank approximation of [@cho_intelligent_2018]. First we use compute an initial low rank estimate with the `AdaptiveInitialize` algorithm. This step is essentially a debiased SVD. We then use the initial solution as a seed for the `AdaptiveImpute` algorithm, a form of iterative SVD thresholding with a data adaptive thresholding parameter.

The input to these algorithms is a partially observed matrix $M$, and $r$, the desired rank of the low-rank approximation. We define $Y$ to be an indicator matrix that tells us whether or not we observed a particular value of $M$.

\begin{algorithm}
\linespread{1.6}\selectfont
\DontPrintSemicolon
\KwIn{$M, Y$ and $r$}
$\hat p \gets \frac{1}{nd} \sum_{i=1}^n \sum_{j=1}^d Y_{ij}$ \;
$\Sigma_{\hat p} \gets M^T M - (1 - \hat p) \diag(M^T M)$ \;
$\Sigma_{t \hat p} \gets M M^T - (1 - \hat p) \diag(M M^T)$ \;
$\hat V_i \gets \bv_i(\Sigma_{\hat p})$ for $i = 1, ..., r$ \;
$\hat U_i \gets \bu_i(\Sigma_{t \hat p})$ for $i = 1, ..., r$ \;
$\tilde \alpha \gets \frac{1}{d - r} \sum_{i=r+1}^d \bl_i (\Sigma_{\hat p})$ \;
$\hat \lambda_i \gets \frac{1}{\hat p} \sqrt{\bl_i (\Sigma_{\hat p}) - \tilde \alpha}$ for $i = 1, ..., r$ \;
$\hat s_i \gets \sign(\langle \hat V_i, \bv_i (M) \rangle) \sign(\langle \hat U_i, \bu_i (M) \rangle)$ for $i = 1, ..., r$ \;
$\hat \lambda_i \gets \hat s_i \cdot \hat \lambda_i$ \;
\Return{$\hat \lambda_i, \hat U_i, \hat V_i$ for $i = 1, ..., r$}\;
\caption{\texttt{AdaptiveInitialize}}
\end{algorithm}

Here $\bu_i, \bl_i, \bv_i$ are functions that return the $i^{th}$ left singular vector, singular value, and right singular value, respectively. $\tilde \alpha$ is the data adaptive thresholding parameter. Note that it is the average of the uncalculated singular values, and is thus positive.

In line 8, $\langle , \rangle$ denotes the Frobenius inner norm. That is, for vectors $x, y$ and matrices $A, B$:

\begin{align}
\langle x, y \rangle &= x^T y = \sum_{i} x_i y_i \\
\langle A, B \rangle &= \sum_{i, j} A_{ij} B_{ij} = \sum_{ij} A \odot B \\
\end{align}

We use $A \odot B$ to mean the elementwise (Hadamard) product of matrices $A$ and $B$.

We can use the output of `AdaptiveInitialize` to construct a low rank approximation to $M$ via:

\begin{align}
\hat M = \sum_{i=1}^r \hat \lambda_i \hat U_i \hat V_i^T
\end{align}

Next up is `AdaptiveImpute`:

\begin{algorithm}
\linespread{1.6}\selectfont
\DontPrintSemicolon
\KwIn{$M, Y, r$ and $\varepsilon > 0$}

$Z^{(1)} \gets \texttt{AdaptiveInitialize}(M, Y, r)$ \;

\Repeat{$\| Z_{t+1} - Z_t \|^2_F / \| Z_{t+1} \|_F$}{
  
  $\tilde M^{(t)} \gets P_\Omega (M) + P_\Omega^\perp (Z_t)$ \;
  $\hat V_i^{(t)} \gets \bv_i(\tilde M^{(t)})$ for $i = 1, ..., r$ \;
  $\hat U_i^{(t)} \gets \bu_i(\tilde M^{(t)})$ for $i = 1, ..., r$ \;
  $\tilde \alpha^{(t)} \gets \frac{1}{d - r} \sum_{i=r+1}^d \bl_i^2 (\tilde M^{(t)})$ \;
  $\hat \lambda_i^{(t)} \gets \sqrt{\bl_i^2 (\tilde M^{(t)}) - \tilde \alpha^{(t)}}$ for $i = 1, ..., r$ \;
  $Z^{(t+1)} \gets \sum_{i=1}^r \hat \lambda_i^{(t)} \hat U_i^{(t)} \hat V_i^{(t)^T}$ \;
  $t \gets t + 1$ \;
}
\Return{$\hat \lambda_i^{(t)}, \hat U_i^{(t)}, \hat V_i^{(t)}$ for $i = 1, ..., r$}\;
\caption{\texttt{AdaptiveImpute}}
\end{algorithm}

Here we again have some new notation. First we define

\begin{align}
P_\Omega(A) &= A \odot Y \\
P_\Omega^\perp (A) &= A \odot (1 - Y)
\end{align}

These are the projecttion of a matrix $A$ onto the observed of $M$ and the unobserved elements of $M$, respectively. Similar $\Omega$ is the set of all pairs $(i, j)$ such that $M_{i, j}$ is observed. $\bl_i^2 (A)$ is a function that returns the $i^{th}$ *squared* singular value of $A$ (i.e. $\bl_i^2 (A) = \left(\bl_i (A)\right)^2$). Finally $\Vert \cdot \Vert_F$ is the Frobenius norm of a matrix.

### Linear algebra facts

Throughout these computations, we will repeatedly use several key facts about eigendecompositions, singular value decompositions (SVD) and the relationship between the two.

https://en.wikipedia.org/wiki/Gramian_matrix X'X -- positive semi-def, so the singular values of M'M are the same as the eigenvalues

question: if A, B positive, is
sum(svd(A - B)$d) == sum(svd(A)$d) - sum(svd(B)$d)

answer: NO! can't split into two easy computations and then combine them
possibly use this to get some sort of bound?

think about what happens as p_hat -> 0

A key observation here is that $M^T M$ and

Fact: sum of squared singular values is `trace(A^T A)`
https://math.stackexchange.com/questions/2281721/sum-of-singular-values-of-a-matrix

Fact: for symmetric positive definite matrices the eigendecomp is equal to the singular value decomp

Fact: sum of eigenvalues of M is equal to trace(M)

Consequence: for pos def symmetric M the sum of the singular values is trace(M) as well

Again computing `alpha` deserves some explanation.

- reference: https://math.stackexchange.com/questions/1463269/how-to-obtain-sum-of-square-of-eigenvalues-without-finding-eigenvalues
- Frobenius norm (A) = trace(crossprod(A))
- TODO: how we know this thing is strictly positive to prevent sqrt() from exploding

```{r}
## STOPPED HERE: WHY ARE the following not the same?
  isSymmetric(sigma_p)
  eigen(sigma_p)$values
  sum(diag(sigma_p))
  sum(svd(sigma_p)$d)
  sum(eigen(sigma_p)$values)
  
  # let's think just about the first term for a moment
  sum(diag(MtM / p_hat^2))
  sum(svd(MtM / p_hat^2)$d)
  
  sum(colSums(M^2 / p_hat^2))
  
  # has some negative eigenvalues
  
  # Fact: for a symmetric matrix, the singular values are the *absolute* values
  # of the eigenvalues
  
  # https://www.mathworks.com/content/dam/mathworks/mathworks-dot-com/moler/eigs.pdf
  
  eigen(sigma_p)$values
  sum(eigen(sigma_p)$values)
  sum(abs(eigen(sigma_p)$values))
  sum(svd(sigma_p)$d)
  sum(abs(diag(sigma_p)))
  
  # those agree so what about the second term
  
  # note to self: alpha should be positive
  # issue karl ran into:
  # https://math.stackexchange.com/questions/381808/sum-of-eigenvalues-and-singular-values
  # how to get the sum of singular values itself (start):
  # https://math.stackexchange.com/questions/569989/sum-of-singular-values-of-ab
  
  # options when sigma_p is not positive definite:
  # - calculate the full SVD
  # - set alpha to zero (don't truncate the singular values)
  # - this lower bounds the average of the remaining singular values
  # -
  
  # positive semi-definite is enough since symmetric and eigen/singular values
  # of zero don't matter
  
  # this is only an issue in the initialization. in the iterative updates
  # we use the squared singular values, which we can more easily calculate
  # the sum of
  
  # ask Karl what he wants to do about this: computing a full SVD is gonna be really expensive.
```

FACT: sum(diag(crossprod(M))) == sum(M^2)

## Reference implementation

```{r}
library(RSpectra)
library(Matrix)
```

<!-- ```{#AdaptiveImpute .R .numberLines} -->

```{r}
adaptive_initialize <- function(M, r) {
  
  # TODO: ignores observed zeros!
  p_hat <- nnzero(M) / prod(dim(M))  # line 1
  
  MtM <- crossprod(M)
  MMt <- tcrossprod(M)
  
  # need to divide by p^2 from Cho et al 2016 to get the "right"
  # singular values / singular values on a comparable scale
  
  # both of these matrices are symmetric, but not necessarily positive
  # this has important implications for the SVD / eigendecomp relationship
  
  sigma_p <- MtM / p_hat^2 - (1 - p_hat) * diag(diag(MtM))  # line 2
  sigma_t <- MMt / p_hat^2 - (1 - p_hat) * diag(diag(MMt))  # line 3
  
  # crossprod() and tcrossprod() return dsCMatrix objects,
  # sparse matrix objects that know they are symmetric
  
  # unfortunately, RSpectra doesn't support dsCMatrix objects,
  # but does support dgCMatrix objects, a class representing sparse
  # but not symmetric matrices
  
  # support for dsCMatrix objects in RSpectra is on the way,
  # which will eliminate the need for the following coercions.
  # see: https://github.com/yixuan/RSpectra/issues/15
  
  sigma_p <- as(sigma_p, "dgCMatrix")
  sigma_t <- as(sigma_t, "dgCMatrix")
  
  svd_p <- svds(sigma_p, r)  # TODO: is eigs_sym() faster?
  svd_t <- svds(sigma_t, r)
  
  v_hat <- svd_p$v  # line 4
  u_hat <- svd_t$u  # line 5
  
  n <- nrow(M)
  d <- ncol(M)
  
  # NOTE: alpha is incorrect due to singular values and eigenvalues
  # being different when sigma_p is not positive
  
  alpha <- (sum(diag(sigma_p)) - sum(svd_p$d)) / (d - r)  # line 6
  lambda_hat <- sqrt(svd_p$d - alpha) / p_hat             # line 7
  
  svd_M <- svds(M, r)
  
  v_sign <- crossprod(rep(1, d), svd_M$v * v_hat)
  u_sign <- crossprod(rep(1, n), svd_M$u * u_hat)
  s_hat <- drop(sign(v_sign * u_sign))
  
  lambda_hat <- lambda_hat * s_hat  # line 8
  
  list(u = u_hat, d = lambda_hat, v = v_hat)
}
```


It's worth commenting on computation of `alpha` and `s_hat`.

When we compute `alpha` in line 20 `adaptive_initialize()`, we don't want to do the full eigendecomposition of $\Sigma_{\hat p}$ since that could take a long time, so we use trick and recall that the trace of a matrix (the sum of it's diagonal elements) equals the sum of all the eigenvalues. Then we subtract off the first $r$ eigenvalues, which we do compute, and are left with $\sum_{i = r + 1}^d \lambda_i(\Sigma_{\hat p})$.

<!-- ```{#AdaptiveImpute .R .numberLines} -->
```{r}
adaptive_impute <- function(M, r, epsilon = 1e-7) {
  
  s <- adaptive_initialize(M, r)
  Z <- s$u %*% diag(s$d) %*% t(s$v)  # line 1
  delta <- Inf
  
  while (delta > epsilon) {
    
    y <- as(M, "lgCMatrix")  # indicator if entry of M observed
    M_tilde <- M + Z * (1 - y)  # line 3
    
    svd_M <- svds(M_tilde, r)
    
    u_hat <- svd_M$u  # line 4
    v_hat <- svd_M$v  # line 5
    
    d <- ncol(M)
    
    alpha <- (sum(M_tilde^2) - sum(svd_M$d^2)) / (d - r)  # line 6
    
    lambda_hat <- sqrt(svd_M$d^2 - alpha)  # line 7
    
    Z_new <- u_hat %*% diag(lambda_hat) %*% t(v_hat)
    
    delta <- sum((Z_new - Z)^2) / sum(Z^2)
    Z <- Z_new
    
    print(glue::glue("delta: {round(delta, 8)}, alpha: {round(alpha, 3)}"))
  }
  
  Z
}
```

Finally we can do a minimal sanity check and see if this code even runs, and see if we are recovering something close-ish to implanted low-rank structure.

```{r}
n <- 500
d <- 100
r <- 5

A <- matrix(runif(n * r, -5, 5), n, r)
B <- matrix(runif(d * r, -5, 5), d, r)
M0 <- A %*% t(B)

err <- matrix(rnorm(n * d), n, d)
Mf <- M0 + err

p <- 0.3
y <- matrix(rbinom(n * d, 1, p), n, d)
dat <- Mf * y

init <- adaptive_initialize(dat, r)
filled <- adaptive_impute(dat, r)
```

## References
