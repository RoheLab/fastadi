[{"path":"/LICENSE.html","id":null,"dir":"","previous_headings":"","what":"MIT License","title":"MIT License","text":"Copyright (c) 2022 fastadi authors Permission hereby granted, free charge, person obtaining copy software associated documentation files (“Software”), deal Software without restriction, including without limitation rights use, copy, modify, merge, publish, distribute, sublicense, /sell copies Software, permit persons Software furnished , subject following conditions: copyright notice permission notice shall included copies substantial portions Software. SOFTWARE PROVIDED “”, WITHOUT WARRANTY KIND, EXPRESS IMPLIED, INCLUDING LIMITED WARRANTIES MERCHANTABILITY, FITNESS PARTICULAR PURPOSE NONINFRINGEMENT. EVENT SHALL AUTHORS COPYRIGHT HOLDERS LIABLE CLAIM, DAMAGES LIABILITY, WHETHER ACTION CONTRACT, TORT OTHERWISE, ARISING , CONNECTION SOFTWARE USE DEALINGS SOFTWARE.","code":""},{"path":"/articles/adaptive-impute.html","id":"abstract","dir":"Articles","previous_headings":"","what":"Abstract","title":"An introduction to AdaptiveInitialize and AdaptiveImpute","text":"TODO: motivate ugh show paper tutorial covering computational details associated computing low-rank rank adaptive imputations matrices described (Cho, Kim, Rohe 2015, 2018). work extends previous data adaptive matrix imputation strategies (Mazumder, Hastie, Tibshirani 2010), better performance eliminating tuning parameters. tutorial proceeds three parts. First, introduce imputation algorithms, useful tidbits linear algebra R package Matrix, use illustrate computations. naive initial implementation eats lots memory, demonstrate memory-efficient implementation. Finally, extend memory-efficient implementation partially observed matrices possibly large number observed zeros. (Bates 2005; Cho, Kim, Rohe 2015, 2018; Maechler Bates 2006; Mazumder, Hastie, Tibshirani 2010; Bro, Acar, Kolda 2007)","code":""},{"path":"/articles/adaptive-impute.html","id":"notation-algorithm","dir":"Articles","previous_headings":"","what":"Notation & Algorithm","title":"An introduction to AdaptiveInitialize and AdaptiveImpute","text":"two steps computing low rank approximation (Cho, Kim, Rohe 2018). First use compute initial low rank estimate AdaptiveInitialize algorithm. step essentially debiased SVD. use initial solution seed AdaptiveImpute algorithm, form iterative SVD thresholding data adaptive thresholding parameter. input algorithms partially observed matrix \\(M\\), \\(r\\), desired rank low-rank approximation. define \\(Y\\) indicator matrix tells us whether observed particular value \\(M\\). \\(\\mathbf{u}_i, \\boldsymbol{\\lambda}_i, \\mathbf{v}_i\\) functions return \\(^{th}\\) left singular vector, singular value, right singular value, respectively. \\(\\tilde \\alpha\\) data adaptive thresholding parameter. Note average uncalculated singular values, thus positive. line 8, \\(\\langle , \\rangle\\) denotes Frobenius inner norm. , vectors \\(x, y\\) matrices \\(, B\\): \\[\\begin{align} \\langle x, y \\rangle &= x^T y = \\sum_{} x_i y_i \\\\ \\langle , B \\rangle &= \\sum_{, j} A_{ij} B_{ij} = \\sum_{ij} \\odot B \\\\ \\end{align}\\] use \\(\\odot B\\) mean elementwise (Hadamard) product matrices \\(\\) \\(B\\). can use output AdaptiveInitialize construct low rank approximation \\(M\\) via: \\[\\begin{align} \\hat M = \\sum_{=1}^r \\hat \\lambda_i \\hat U_i \\hat V_i^T \\end{align}\\] Next AdaptiveImpute: new notation. First define \\[\\begin{align} P_\\Omega() &= \\odot Y \\\\ P_\\Omega^\\perp () &= \\odot (1 - Y) \\end{align}\\] projecttion matrix \\(\\) onto observed \\(M\\) unobserved elements \\(M\\), respectively. Similar \\(\\Omega\\) set pairs \\((, j)\\) \\(M_{, j}\\) observed. \\(\\boldsymbol{\\lambda}_i^2 ()\\) function returns \\(^{th}\\) squared singular value \\(\\) (.e. \\(\\boldsymbol{\\lambda}_i^2 () = \\left(\\boldsymbol{\\lambda}_i ()\\right)^2\\)). Finally \\(\\Vert \\cdot \\Vert_F\\) Frobenius norm matrix.","code":""},{"path":"/articles/adaptive-impute.html","id":"linear-algebra-facts","dir":"Articles","previous_headings":"Notation & Algorithm","what":"Linear algebra facts","title":"An introduction to AdaptiveInitialize and AdaptiveImpute","text":"Throughout computations, repeatedly use several key facts eigendecompositions, singular value decompositions (SVD) relationship two. https://en.wikipedia.org/wiki/Gramian_matrix X’X – positive semi-def, singular values M’M eigenvalues question: , B positive, sum(svd(- B)\\(d) == sum(svd()\\)d) - sum(svd(B)$d) answer: ! can’t split two easy computations combine possibly use get sort bound? think happens p_hat -> 0 key observation \\(M^T M\\) Fact: sum squared singular values trace(^T ) https://math.stackexchange.com/questions/2281721/sum--singular-values---matrix Fact: symmetric positive definite matrices eigendecomp equal singular value decomp Fact: sum eigenvalues M equal trace(M) Consequence: pos def symmetric M sum singular values trace(M) well computing alpha deserves explanation. reference: https://math.stackexchange.com/questions/1463269/--obtain-sum--square--eigenvalues-without-finding-eigenvalues Frobenius norm () = trace(crossprod()) TODO: know thing strictly positive prevent sqrt() exploding FACT: sum(diag(crossprod(M))) == sum(M^2)","code":"## STOPPED HERE: WHY ARE the following not the same?   isSymmetric(sigma_p)   eigen(sigma_p)$values   sum(diag(sigma_p))   sum(svd(sigma_p)$d)   sum(eigen(sigma_p)$values)      # let's think just about the first term for a moment   sum(diag(MtM / p_hat^2))   sum(svd(MtM / p_hat^2)$d)      sum(colSums(M^2 / p_hat^2))      # has some negative eigenvalues      # Fact: for a symmetric matrix, the singular values are the *absolute* values   # of the eigenvalues      # https://www.mathworks.com/content/dam/mathworks/mathworks-dot-com/moler/eigs.pdf      eigen(sigma_p)$values   sum(eigen(sigma_p)$values)   sum(abs(eigen(sigma_p)$values))   sum(svd(sigma_p)$d)   sum(abs(diag(sigma_p)))      # those agree so what about the second term      # note to self: alpha should be positive   # issue karl ran into:   # https://math.stackexchange.com/questions/381808/sum-of-eigenvalues-and-singular-values   # how to get the sum of singular values itself (start):   # https://math.stackexchange.com/questions/569989/sum-of-singular-values-of-ab      # options when sigma_p is not positive definite:   # - calculate the full SVD   # - set alpha to zero (don't truncate the singular values)   # - this lower bounds the average of the remaining singular values   # -      # positive semi-definite is enough since symmetric and eigen/singular values   # of zero don't matter      # this is only an issue in the initialization. in the iterative updates   # we use the squared singular values, which we can more easily calculate   # the sum of      # ask Karl what he wants to do about this: computing a full SVD is gonna be really expensive."},{"path":"/articles/adaptive-impute.html","id":"reference-implementation","dir":"Articles","previous_headings":"","what":"Reference implementation","title":"An introduction to AdaptiveInitialize and AdaptiveImpute","text":"’s worth commenting computation alpha s_hat. compute alpha line 20 adaptive_initialize(), don’t want full eigendecomposition \\(\\Sigma_{\\hat p}\\) since take long time, use trick recall trace matrix (sum ’s diagonal elements) equals sum eigenvalues. subtract first \\(r\\) eigenvalues, compute, left \\(\\sum_{= r + 1}^d \\lambda_i(\\Sigma_{\\hat p})\\). Finally can minimal sanity check see code even runs, see recovering something close-ish implanted low-rank structure.","code":"library(RSpectra) library(Matrix) adaptive_initialize <- function(M, r) {      # TODO: ignores observed zeros!   p_hat <- nnzero(M) / prod(dim(M))  # line 1      MtM <- crossprod(M)   MMt <- tcrossprod(M)      # need to divide by p^2 from Cho et al 2016 to get the \"right\"   # singular values / singular values on a comparable scale      # both of these matrices are symmetric, but not necessarily positive   # this has important implications for the SVD / eigendecomp relationship      sigma_p <- MtM / p_hat^2 - (1 - p_hat) * diag(diag(MtM))  # line 2   sigma_t <- MMt / p_hat^2 - (1 - p_hat) * diag(diag(MMt))  # line 3      # crossprod() and tcrossprod() return dsCMatrix objects,   # sparse matrix objects that know they are symmetric      # unfortunately, RSpectra doesn't support dsCMatrix objects,   # but does support dgCMatrix objects, a class representing sparse   # but not symmetric matrices      # support for dsCMatrix objects in RSpectra is on the way,   # which will eliminate the need for the following coercions.   # see: https://github.com/yixuan/RSpectra/issues/15      sigma_p <- as(sigma_p, \"dgCMatrix\")   sigma_t <- as(sigma_t, \"dgCMatrix\")      svd_p <- svds(sigma_p, r)  # TODO: is eigs_sym() faster?   svd_t <- svds(sigma_t, r)      v_hat <- svd_p$v  # line 4   u_hat <- svd_t$u  # line 5      n <- nrow(M)   d <- ncol(M)      # NOTE: alpha is incorrect due to singular values and eigenvalues   # being different when sigma_p is not positive      alpha <- (sum(diag(sigma_p)) - sum(svd_p$d)) / (d - r)  # line 6   lambda_hat <- sqrt(svd_p$d - alpha) / p_hat             # line 7      svd_M <- svds(M, r)      v_sign <- crossprod(rep(1, d), svd_M$v * v_hat)   u_sign <- crossprod(rep(1, n), svd_M$u * u_hat)   s_hat <- drop(sign(v_sign * u_sign))      lambda_hat <- lambda_hat * s_hat  # line 8      list(u = u_hat, d = lambda_hat, v = v_hat) } adaptive_impute <- function(M, r, epsilon = 1e-7) {      s <- adaptive_initialize(M, r)   Z <- s$u %*% diag(s$d) %*% t(s$v)  # line 1   delta <- Inf      while (delta > epsilon) {          y <- as(M, \"lgCMatrix\")  # indicator if entry of M observed     M_tilde <- M + Z * (1 - y)  # line 3          svd_M <- svds(M_tilde, r)          u_hat <- svd_M$u  # line 4     v_hat <- svd_M$v  # line 5          d <- ncol(M)          alpha <- (sum(M_tilde^2) - sum(svd_M$d^2)) / (d - r)  # line 6          lambda_hat <- sqrt(svd_M$d^2 - alpha)  # line 7          Z_new <- u_hat %*% diag(lambda_hat) %*% t(v_hat)          delta <- sum((Z_new - Z)^2) / sum(Z^2)     Z <- Z_new          print(glue::glue(\"delta: {round(delta, 8)}, alpha: {round(alpha, 3)}\"))   }      Z } n <- 500 d <- 100 r <- 5  A <- matrix(runif(n * r, -5, 5), n, r) B <- matrix(runif(d * r, -5, 5), d, r) M0 <- A %*% t(B)  err <- matrix(rnorm(n * d), n, d) Mf <- M0 + err  p <- 0.3 y <- matrix(rbinom(n * d, 1, p), n, d) dat <- Mf * y  init <- adaptive_initialize(dat, r) filled <- adaptive_impute(dat, r)"},{"path":"/articles/adaptive-impute.html","id":"references","dir":"Articles","previous_headings":"","what":"References","title":"An introduction to AdaptiveInitialize and AdaptiveImpute","text":"Bates, Douglas. 2005. “Introduction Matrix Package.” https://cran.r-project.org/web/packages/Matrix/vignettes/Introduction.pdf. Bro, Rasmus, Evrim Acar, Tamara G. Kolda. 2007. “Resolving Sign Ambiguity Singular Value Decomposition,” 18. Cho, Juhee, Donggyu Kim, Karl Rohe. 2015. “Asymptotic Theory Estimating Singular Vectors Values Partially-Observed Low Rank Matrix Noise.” arXiv:1508.05431 [Stat], August. http://arxiv.org/abs/1508.05431. ———. 2018. “Intelligent Initialization Adaptive Thresholding Iterative Matrix Completion; Statistical Algorithmic Theory Adaptive-Impute.” Journal Computational Graphical Statistics, September, 1–26. https://doi.org/10.1080/10618600.2018.1518238. Maechler, Martin, Douglas Bates. 2006. “2nd Introduction Matrix Package.” https://cran.r-project.org/web/packages/Matrix/vignettes/Intro2Matrix.pdf. Mazumder, Rahul, Trevor Hastie, Robert Tibshirani. 2010. “Spectral Regularization Algorithms Learning Large Incomplete Matrices.” https://web.stanford.edu/~hastie/Papers/mazumder10a.pdf.","code":""},{"path":"/articles/efficient-sparse-computation.html","id":"low-rank-implementation","dir":"Articles","previous_headings":"","what":"Low-rank implementation","title":"Using memory-efficient sparse computations","text":"reference implementation problems. data matrix \\(M\\) gets larger, can longer fit dense representation \\(\\hat M\\) \\(Z^{(t)}\\) memory. Instead, need work just low rank components \\(\\hat \\lambda, \\hat U\\) \\(\\hat V\\). leads us following implementation: eigen_helper() ? Describe return object (also svds) Now check Mx works","code":"low_rank_adaptive_initialize <- function(M, r) {      M <- as(M, \"dgCMatrix\")      p_hat <- nnzero(M) / prod(dim(M))  # line 1      # NOTE: skip explicit computation of line 2   # NOTE: skip explicit computation of line 3      eig_p <- eigen_helper(M, r)   eig_t <- eigen_helper(t(M), r)      lr_v_hat <- eig_p$vectors  # line 4   lr_u_hat <- eig_t$vectors  # line 5      d <- ncol(M)   n <- nrow(M)      # NOTE: alpha is again incorrect since we work with eigenvalues   # rather than singular values here   sum_eigen_values <- sum(M@x^2) / p^2 - (1 - p) * sum(colSums(M^2))   lr_alpha <- (sum_eigen_values - sum(eig_p$values)) / (d - r)  # line 6      lr_lambda_hat <- sqrt(eig_p$values - lr_alpha) / p_hat  # line 7      # TODO: Karl had another sign computation here that he said was faster   # but it wasn't documented anywhere, so I'm going with what was in the    # paper      lr_svd_M <- svds(M, r)      # v_hat is d by r   lr_v_sign <- crossprod(rep(1, d), lr_svd_M$v * lr_v_hat)   lr_u_sign <- crossprod(rep(1, n), lr_svd_M$u * lr_u_hat)   lr_s_hat <- c(sign(lr_v_sign * lr_u_sign))  # line 8      lr_lambda_hat <- lr_lambda_hat * lr_s_hat      list(u = lr_u_hat, d = lr_lambda_hat, v = lr_v_hat) } # Take the eigendecomposition of t(M) %*% M - (1 - p) * diag(t(M) %*% M) # using sparse computations only eigen_helper <- function(M, r) {   eigs_sym(     Mx, r,     n = ncol(M),     args = list(       M = M,       p = nnzero(M) / prod(dim(M))     )   ) }  # compute (t(M) %*% M / p^2 - (1 - p) * diag(diag(t(M) %*% M))) %*% x # using sparse operations  # TODO: divide the second term by p^2 like in the reference implementatio Mx <- function(x, args) {   drop(     crossprod(args$M, args$M %*% x) / args$p^2 - (1 - args$p) * Diagonal(ncol(args$M), colSums(args$M^2)) %*% x   ) } x <- rnorm(12) p <- 0.3 out <- (t(M) %*% M / p^2 - (1 - p) * diag(diag(t(M) %*% M))) %*% x out2 <- Mx(x, args = list(M = M, p = p)) all.equal(as.matrix(out), as.matrix(out))"},{"path":"/articles/efficient-sparse-computation.html","id":"check-the-computation-of-alpha-for-p-s-d--matrices","dir":"Articles","previous_headings":"","what":"check the computation of alpha for p.s.d. matrices","title":"Using memory-efficient sparse computations","text":"TODO: update alg description divide \\(p^2\\) get right singular values Quickly check components works try code integrates together Finally, sanity check comparing reference implementation. don’t agree, isn’t great:","code":"S <- toeplitz((10:1)/10) set.seed(11) R <- rWishart(1, 20, S)[, ,1] R  sum(diag(R)) sum(eigen(R)$values) sum(svd(R)$d)  p <- 0.5  X <- crossprod(R) - (1 - p) * diag(crossprod(R))   sum(diag(X)) sum(eigen(X)$values) sum(svd(X)$d) M <- ml100k s <- svd(M) # nuclear norm sum(s$d) norm(M, type = \"F\")  r <- 5  d <- ncol(M) n <- nrow(M)  alpha <- (sum(s$d) - sum(s$d[1:r])) / (d - r) alpha  s$d[1:r] lr_init <- sparse_adaptive_initialize(dat, r)  # some weird stuff is happening with the singular values but I'm # going to not worry about it for the time being  equal_svds(init, lr_init)"},{"path":"/articles/efficient-sparse-computation.html","id":"space-efficient-adaptive-impute","dir":"Articles","previous_headings":"check the computation of alpha for p.s.d. matrices","what":"Space-efficient adaptive impute","title":"Using memory-efficient sparse computations","text":"TODO: figure actual space complexity Recall algorithm looks like \\[\\begin{align} \\hat M = \\sum_{=1}^r \\hat s_i  \\hat \\lambda_i \\hat U_i \\hat V_i^T \\end{align}\\] Now need two things: SVD \\(\\tilde M^{(t)}\\) (Certain sums ) squared singular values.","code":""},{"path":"/articles/efficient-sparse-computation.html","id":"sums-of-squared-singular-values","dir":"Articles","previous_headings":"check the computation of alpha for p.s.d. matrices > Space-efficient adaptive impute","what":"Sums of squared singular values","title":"Using memory-efficient sparse computations","text":"matrix \\(\\), sum squared singular values (denoted \\(\\lambda_i\\)) equals squared frobenius norm: \\[\\begin{align} \\sum_{=1}^{\\min(n, d)} \\lambda_i^2 = ||||_F^2 = \\mathop{\\mathrm{trace}}(^T ) \\end{align}\\] Also note \\[\\begin{align} ||+ B||_F^2 = ||||_F^2 + ||B||_F^2 + 2 \\cdot \\langle , B \\rangle_F \\end{align}\\] Now consider \\(\\tilde M^{(t)}\\). Suppose unobserved values \\(M\\) set zero, case \\(M\\) stored sparse matrix representation \\[\\begin{align} \\tilde M^{(t)} &= P_\\Omega(M) + P_\\Omega^\\perp (Z_t) \\\\ &= P_\\Omega(M) + P_\\Omega^\\perp \\left(   \\sum_{=1}^r \\hat \\lambda_i^{(t)} \\hat U_i^{(t)} \\hat V_i^{(t)^T}   \\right) \\end{align}\\] Now need \\[\\begin{align} ||\\tilde M^{(t)}||_F^2  &= \\left \\Vert   P_\\Omega(M) + P_\\Omega^\\perp \\left(     \\sum_{=1}^r \\hat \\lambda_i^{(t)} \\hat U_i^{(t)} \\hat V_i^{(t)^T}     \\right)   \\right \\Vert_F^2 \\\\ &= \\left \\Vert P_\\Omega(M) \\right \\Vert_F^2    + \\left \\Vert P_\\Omega^\\perp \\left(       \\sum_{=1}^r \\hat \\lambda_i^{(t)} \\hat U_i^{(t)} \\hat V_i^{(t)^T}     \\right)   \\right \\Vert_F^2   + 2 \\cdot \\left \\langle P_\\Omega(M), P_\\Omega^\\perp \\left(       \\sum_{=1}^r \\hat \\lambda_i^{(t)} \\hat U_i^{(t)} \\hat V_i^{(t)^T}     \\right) \\right \\rangle_F \\\\ &= \\left \\Vert P_\\Omega(M) \\right \\Vert_F^2    + \\left \\Vert P_\\Omega^\\perp \\left(       \\sum_{=1}^r \\hat \\lambda_i^{(t)} \\hat U_i^{(t)} \\hat V_i^{(t)^T}     \\right)   \\right \\Vert_F^2 \\end{align}\\] cancellation final line follows \\[\\begin{align} \\left \\langle P_\\Omega(M), P_\\Omega^\\perp   \\left(     \\sum_{=1}^r \\hat \\lambda_i^{(t)} \\hat U_i^{(t)} \\hat V_i^{(t)^T}   \\right) \\right \\rangle_F = \\sum_{, j} P_\\Omega(M)_{ij} \\cdot P_\\Omega^\\perp (Z_t)_{ij} = \\sum_{, j} 0 = 0 \\end{align}\\] Now need one trick, \\[\\begin{align} \\left \\Vert Z_t \\right \\Vert_F^2 = \\left \\Vert P_\\Omega (Z_t) + P_\\Omega^\\perp (Z_t) \\right \\Vert_F^2 = \\left \\Vert P_\\Omega (Z_t) \\right \\Vert_F^2 +    \\left \\Vert P_\\Omega^\\perp (Z_t) \\right \\Vert_F^2 \\end{align}\\] \\[\\begin{align} \\left \\Vert P_\\Omega^\\perp (Z_t) \\right \\Vert_F^2 = \\left \\Vert Z_t \\right \\Vert_F^2   - \\left \\Vert P_\\Omega (Z_t) \\right \\Vert_F^2 \\\\ = \\sum_{= 1}^r \\lambda_i^2 - \\left \\Vert Z_t \\odot Y \\right \\Vert_F^2 \\end{align}\\] Putting together see \\[\\begin{align} ||\\tilde M^{(t)}||_F^2  &= \\left \\Vert P_\\Omega(M) \\right \\Vert_F^2    + \\left \\Vert P_\\Omega^\\perp \\left(       \\sum_{=1}^r \\hat \\lambda_i^{(t)} \\hat U_i^{(t)} \\hat V_i^{(t)^T}     \\right)   \\right \\Vert_F^2 \\\\ &= \\left \\Vert M \\right \\Vert_F^2 +    \\sum_{= 1}^r \\lambda_i^2 -    \\left \\Vert Z_t \\odot Y \\right \\Vert_F^2 \\end{align}\\] code sparse matrix M list s elements SVD. first Frobenious norm quick calculate, sure calculate two frobenius norms. Test , take eigendecomp just need able \\(Mx\\). take SVD, need? matrix vector matrix transpose vector multiplication","code":"# s is a matrix defined in terms of it's svd # G is a sparse matrix # compute only elements of U %*% diag(d) %*% t(V) only on non-zero elements of G # G and U %*% t(V) must have same dimensions  # maybe call this svd_perp? svd_perp <- function(s, mask) {      # note: must be dgTMatrix to get column indexes j larger   # what if we used dlTMatrix here?   m <- as(mask, \"dgTMatrix\")      # the indices for which we want to compute the matrix multiplication   # turn zero based indices into one based indices   i <- m@i + 1   j <- m@j + 1    # gets rows and columns of U and V to multiply, then multiply   ud <- s$u %*% diag(s$d)   left <- ud[i, ]   right <- s$v[j, ]    # compute inner products to get elements of U %*% t(V)   uv <- rowSums(left * right)    # NOTE: specify dimensions just in case   sparseMatrix(i = i, j = j, x = uv, dims = dim(mask)) } set.seed(17)  M <- rsparsematrix(8, 12, nnz = 30) s <- svds(M, 5)  y <- as(M, \"lgCMatrix\")  Z <- s$u %*% diag(s$d) %*% t(s$v)  all.equal(   svd_perp(s, M),   Z * y ) set.seed(17) r <- 5  M <- rsparsematrix(8, 12, nnz = 30) y <- as(M, \"lgCMatrix\")  s <- svds(M, r) Z <- s$u %*% diag(s$d) %*% t(s$v)  M_tilde <- M + Z * (1 - y)  # dense!  Z_perp <- svd_perp(s, M) sum_singular_squared <- sum(M@x^2) + sum(s$d^2) - sum(Z_perp@x^2)  all.equal(   sum(svd(M_tilde)$d^2),   sum_singular_squared )"},{"path":"/articles/efficient-sparse-computation.html","id":"svd-of-m-tilde","dir":"Articles","previous_headings":"check the computation of alpha for p.s.d. matrices > Space-efficient adaptive impute","what":"SVD of M tilde","title":"Using memory-efficient sparse computations","text":"’re done first sanity check function interface. Let \\(x\\) vector. Now want calculate \\[\\begin{align} \\tilde M^{(t)} x    &= \\left[ P_\\Omega(M) + P_\\Omega^\\perp (Z_t) \\right] x \\\\   &= \\left[ P_\\Omega(M) -     P_\\Omega (Z_t) +     P_\\Omega (Z_t) +     P_\\Omega^\\perp (Z_t) \\right] x \\\\   &= P_\\Omega(M - Z_t) x + Z_t x \\end{align}\\] can think \\(R_t \\equiv P_\\Omega(M - Z_t)\\) “residuals” sorts. Crucially, \\(R_t\\) sparse, \\[\\begin{align} Z_t x &= (\\hat U    \\mathop{\\mathrm{diag}}(\\hat \\lambda_1, ..., \\hat \\lambda_r)   \\hat V^t) x \\\\   &= (\\hat U    (\\mathop{\\mathrm{diag}}(\\hat \\lambda_1, ..., \\hat \\lambda_r)   (\\hat V^t x))) \\end{align}\\] now memory requirement computation reduced two sparse matrix vector multiplications, rather fitting dense matrix \\(P_\\Omega^\\perp (Z_t)\\) memory. Similarly, transpose, \\[\\begin{align} \\tilde M^{{(t)}^T} x    &= \\left[ P_\\Omega(M) + P_\\Omega^\\perp (Z_t) \\right]^T x \\\\   &= \\left[ P_\\Omega(M) -     P_\\Omega (Z_t) +     P_\\Omega (Z_t) +     P_\\Omega^\\perp (Z_t) \\right]^T x \\\\   &= P_\\Omega(M - Z_t)^T x + Z_t^T x \\end{align}\\] leads us second, less memory intensive implementation Ax() Atx(): \\[\\begin{align} \\texttt{MtM} = \\tilde M^{{(t)}^T} \\tilde M^{(t)} \\end{align}\\]","code":"set.seed(17) r <- 5  M <- rsparsematrix(8, 12, nnz = 30) y <- as(M, \"lgCMatrix\")  s <- svds(M, r) Z <- s$u %*% diag(s$d) %*% t(s$v)  M_tilde <- M + Z * (1 - y)  # dense!  svd_M_tilde <- svds(M_tilde, r) svd_M_tilde Ax <- function(x, args) {   drop(M_tilde %*% x) }  Atx <- function(x, args) {   drop(t(M_tilde) %*% x) }  # is eigs_sym() with a two-sided multiply faster? args <- list(u = s$u, d = s$d, v = s$v, m = M) test1 <- svds(Ax, k = r, Atrans = Atx, dim = dim(M), args = args)  test1 svd_M_tilde  all.equal(   svd_M_tilde,   test1 ) # input: M, Z_t as a low-rank SVD list s  R <- M - svd_perp(s, M)  # residual matrix args <- list(u = s$u, d = s$d, v = s$v, R = R)  Ax <- function(x, args) {   drop(args$R %*% x + args$u %*% diag(args$d) %*% crossprod(args$v, x)) }  Atx <- function(x, args) {   # TODO: can we use a crossprod() for the first multiplication here?   drop(t(args$R) %*% x + args$v %*% diag(args$d) %*% crossprod(args$u, x)) }  # is eigs_sym() with a two-sided multiply faster? test2 <- svds(Ax, k = r, Atrans = Atx, dim = dim(M), args = args)  all.equal(   svd_M_tilde,   test2 ) relative_f_norm_change <- function(s_new, s) {   # TODO: don't do the dense calculation here      Z_new <- s_new$u %*% diag(s_new$d) %*% t(s_new$v)   Z_new <- s$u %*% diag(s$d) %*% t(s$v)      sum((Z_new - Z)^2) / sum(Z^2) } sparse_adaptive_impute <- function(M, r, epsilon = 1e-03) {    # coerce M to sparse matrix such that we use sparse operations   M <- as(M, \"dgCMatrix\")      # low rank svd-like object, s ~ Z_1   s <- sparse_adaptive_initialize(M, r)  # line 1   delta <- Inf   d <- ncol(M)   norm_M <- sum(M@x^2)    while (delta > epsilon) {          # update s: lines 4 and 5     # take the SVD of M-tilde          R <- M - svd_perp(s, M)  # residual matrix     args <- list(u = s$u, d = s$d, v = s$v, R = R)          s_new <- svds(Ax, k = r, Atrans = Atx, dim = dim(M), args = args)          MtM <- norm_M + sum(s_new$d^2) - sum(svd_perp(s_new, M)^2)     alpha <- (sum(MtM) - sum(s_new$d^2)) / (d - r)  # line 6          s_new$d <- sqrt(s_new$d^2 - alpha)  # line 7          # NOTE: skip explicit computation of line 8     delta <- relative_f_norm_change(s_new, s)          s <- s_new          print(glue::glue(\"delta: {round(delta, 8)}, alpha: {round(alpha, 3)}\"))   }      s } out <- sparse_adaptive_impute(M, r) out"},{"path":"/articles/efficient-sparse-computation.html","id":"extension-to-a-mixture-of-observed-and-unobserved-missingness","dir":"Articles","previous_headings":"check the computation of alpha for p.s.d. matrices","what":"Extension to a mixture of observed and unobserved missingness","title":"Using memory-efficient sparse computations","text":"originally solving optimization vaguely form \\[\\begin{align} \\left \\Vert M - \\hat M \\right \\Vert_F^2 \\end{align}\\] \\(M\\) partially observed matrix. now, let \\(M' = Y M\\) \\(Y\\) indicator whether \\(M\\) observed. Typically setup like \\[\\begin{align} M =   \\begin{bmatrix}     \\cdot & \\cdot & 3     & 1     & \\cdot \\\\     3     & \\cdot & \\cdot & 8     & \\cdot \\\\     \\cdot & -1    & \\cdot & \\cdot & \\cdot \\\\     \\cdot & \\cdot & \\cdot & \\cdot & \\cdot \\\\     \\cdot & 2     & \\cdot & \\cdot & \\cdot \\\\     5     & \\cdot & 7     & \\cdot & 4   \\end{bmatrix} , \\qquad Y =    \\begin{bmatrix}     \\cdot & \\cdot & 1     & 1     & \\cdot \\\\     1     & \\cdot & \\cdot & 1     & \\cdot \\\\     \\cdot & 1     & \\cdot & \\cdot & \\cdot \\\\     \\cdot & \\cdot & \\cdot & \\cdot & \\cdot \\\\     \\cdot & 1     & \\cdot & \\cdot & \\cdot \\\\     1     & \\cdot & 1     & \\cdot & 1   \\end{bmatrix} \\end{align}\\] symbol \\(\\cdot\\) means entry matrix unobserved. now , continuing represent \\(M\\) sparse matrix zero entries, observe bunch zeros. might know upper triangule \\(M\\) structurally missing zeros observed missing. zeros primarily important affect residuals calculations. particular case, take multiplication \\(M\\), sparse operation, make dense operation. point becomes useful introduce additional notation. Let \\(\\tilde \\Omega\\) set indicies \\((, j)\\) \\(M_{, j}\\) non-zero. Observe \\(\\tilde \\Omega \\subset \\Omega\\). \\(P_{\\tilde \\Omega} () = P_\\Omega ()\\). \\[\\begin{align} M =   \\begin{bmatrix}     0     & 0     & 3     & 1     & 0 \\\\     3     & 0     & 0     & 8     & 0 \\\\     \\cdot & -1    & 0     & 0     & 0 \\\\     \\cdot & \\cdot & \\cdot & 0     & 0 \\\\     \\cdot & 2     & \\cdot & \\cdot & 0 \\\\     5     & \\cdot & 7     & \\cdot & 4   \\end{bmatrix} , \\qquad Y =    \\begin{bmatrix}     1     & 1     & 1     & 1     & 1 \\\\     1     & 1     & 1     & 1     & 1 \\\\     \\cdot & 1     & 1     & 1     & 1 \\\\     \\cdot & \\cdot & \\cdot & 1     & 1 \\\\     \\cdot & 1     & \\cdot & \\cdot & 1 \\\\     1     & \\cdot & 1     & \\cdot & 1   \\end{bmatrix} , \\qquad M' =   \\begin{bmatrix}     \\cdot & \\cdot & 3     & 1     & \\cdot \\\\     3     & \\cdot & \\cdot & 8     & \\cdot \\\\     \\cdot & -1    & \\cdot & \\cdot & \\cdot \\\\     \\cdot & \\cdot & \\cdot & \\cdot & \\cdot \\\\     \\cdot & 2     & \\cdot & \\cdot & \\cdot \\\\     5     & \\cdot & 7     & \\cdot & 4   \\end{bmatrix} \\end{align}\\] Temporary scratch presentation \\[\\begin{align} =   \\begin{bmatrix}     0 & 0 & 1 & 1 & 0 \\\\     ? & 0 & 0 & 1 & 0 \\\\     ? & ? & 0 & 0 & 0 \\\\     ? & ? & ? & 0 & 1 \\\\     ? & ? & ? & ? & 0 \\\\     ? & ? & ? & ? & 0   \\end{bmatrix} \\end{align}\\] now need figure calculate \\[\\begin{align} \\tilde M^{(t)} x    &= \\left[ P_\\Omega(M) + P_\\Omega^\\perp (Z_t) \\right] x \\\\   &= \\left[ P_\\Omega(M) -     P_\\Omega (Z_t) +     P_\\Omega (Z_t) +     P_\\Omega^\\perp (Z_t) \\right] x \\\\   &= P_\\Omega(M)  - P_\\Omega(Z_t) x + Z_t x \\\\   &= P_{\\tilde \\Omega}(M)  - P_\\Omega(Z_t) x + Z_t x \\end{align}\\] already know calculate \\(P_{\\tilde \\Omega}(M)\\) \\(Z_t x\\) using sparse operations, ’re left \\(P_\\Omega(Z_t) x\\). Note \\(P_\\Omega(Z_t) x \\neq P_{\\tilde \\Omega} (Z_t) x\\) since \\(Z_t\\) necessarily zero \\(\\tilde \\Omega^\\perp\\). words \\(P_\\Omega^\\perp (Z_t) \\neq Z_t\\). \\(Y\\) dense, way avoid paying computational cost dense near dense computation. primary concern fitting \\(Y\\) memory large datasets. issue \\(Y\\) dense discernible structure permits compact representation. can fit \\(Y\\) memory, can low-rank computation, calculating elements \\(Z^{(t)}_{ij}\\) \\(Y_{ij} = 1\\). \\(Y\\) stored vector row indices together vector column indices (plus information dimension), can write computation : point ’s worth writing explicitly calculate \\(Z^{(t)}_{ij}\\). \\[\\begin{align} Z^{(t)}_{ij}  &= \\left( \\sum_{\\ell=1}^r \\hat U_\\ell \\hat d_\\ell \\hat V_\\ell^T \\right)_{ij} \\end{align}\\] visual reminder, looks like (using \\(\\mathop{\\mathrm{diag}}(\\hat d)\\) \\(\\hat \\Sigma\\) somewhat interchangeably ) \\[\\begin{align} Z^{(t)} = \\hat U \\mathop{\\mathrm{diag}}(\\hat d) \\, \\hat V^T =  \\begin{bmatrix}   U_1 & U_2 & \\hdots & U_r \\\\ \\end{bmatrix} \\begin{bmatrix}   d_{1} & 0 & \\hdots & 0 \\\\   0 & d_{2} & \\hdots & 0 \\\\   \\vdots & \\vdots & \\ddots & \\vdots \\\\   0 & 0 & \\hdots & d_{r} \\end{bmatrix} \\begin{bmatrix}   V_1^T \\\\   V_2^T \\\\   \\vdots \\\\   V_r^T \\end{bmatrix} \\end{align}\\] \\[\\begin{align} \\begin{bmatrix}   U_{11} & U_{12} & \\hdots & U_{1r} \\\\   U_{21} & U_{22} & \\hdots & U_{2r} \\\\   \\vdots & \\vdots & \\ddots & \\vdots \\\\   U_{n1} & U_{n2} & \\hdots & U_{nr} \\end{bmatrix} \\begin{bmatrix}   d_{1} & 0 & \\hdots & 0 \\\\   0 & d_{2} & \\hdots & 0 \\\\   \\vdots & \\vdots & \\ddots & \\vdots \\\\   0 & 0 & \\hdots & d_{r} \\end{bmatrix} \\begin{bmatrix}   V_{11} & V_{21} & \\hdots & V_{d1} \\\\   V_{12} & V_{22} & \\hdots & V_{d2} \\\\   \\vdots & \\vdots & \\ddots & \\vdots \\\\   V_{1r} & V_{2r} & \\hdots & V_{dr} \\end{bmatrix} \\end{align}\\] n x d = (n x r) x (r x r) x (r x d) (r x d) transpose \\[\\begin{align} \\begin{bmatrix}   U_{11} & U_{12} & \\hdots & U_{1r} \\\\   U_{21} & U_{22} & \\hdots & U_{2r} \\\\   \\vdots & \\vdots & \\ddots & \\vdots \\\\   U_{n1} & U_{n2} & \\hdots & U_{nr} \\end{bmatrix} \\begin{bmatrix}   d_{1} & 0 & \\hdots & 0 \\\\   0 & d_{2} & \\hdots & 0 \\\\   \\vdots & \\vdots & \\ddots & \\vdots \\\\   0 & 0 & \\hdots & d_{r} \\end{bmatrix} \\begin{bmatrix}   V_{11} & V_{21} & \\hdots & V_{d1} \\\\   V_{12} & V_{22} & \\hdots & V_{d2} \\\\   \\vdots & \\vdots & \\ddots & \\vdots \\\\   V_{1r} & V_{2r} & \\hdots & V_{dr} \\end{bmatrix} \\end{align}\\] Also never remember anything, recall \\(Z\\) \\(n \\times d\\), \\(x\\) \\(d \\times 1\\) \\(Zx\\) \\(n \\times 1\\): \\[\\begin{align} (Zx)_i = \\sum_{j=1}^d Z_{ij} \\cdot x_j \\end{align}\\] (recall \\(X_i\\) always refers \\(^{th}\\) column \\(X\\) document) gonna painfully slow R rewrite C++ TODO: tranpose multiplication average singular value calculation Recall calculate average singular value want \\[\\begin{align} ||\\tilde M^{(t)}||_F^2 &= \\left \\Vert M \\right \\Vert_F^2 +    \\sum_{= 1}^r \\lambda_i^2 -    \\left \\Vert Z_t \\odot Y \\right \\Vert_F^2 \\end{align}\\] computationally intensive part let’s write Armadillo well","code":"M <- Matrix(   rbind(     c(0, 0, 3, 1, 0),     c(3, 0, 0, 8, 0),     c(0, -1, 0, 0, 0),     c(0, 0, 0, 0, 0),     c(0, 2, 0, 0, 0),     c(5, 0, 7, 0, 4)   ) )  Y <- rbind(   c(1, 1, 1, 1, 1),   c(1, 1, 1, 1, 1),   c(0, 1, 1, 1, 1),   c(0, 0, 1, 1, 1),   c(0, 1, 0, 1, 1),   c(1, 0, 1, 0, 1) )  s <- svds(M, 2)  Y <- as(Y, \"CsparseMatrix\")  # triplet form # compressed column matrix form even better but don't # understand the format Y <- as(Y, \"lgCMatrix\") Y <- as(Y, \"lgTMatrix\")  # ugh: RcppArmadillo only supports dgTMatrix rather than # lgTMatrix which is somewhat unfortunate  # link: https://cran.r-project.org/web/packages/RcppArmadillo/vignettes/RcppArmadillo-sparseMatrix.pdf  Y  x <- rnorm(5)  # want to calculate Z <- s$u %*% diag(s$d) %*% t(s$v) out <- drop((Z * Y) %*% x) out # mask as a pair list # L and Z / svd are both n x d matrices # x is a d x 1 matrix / vector masked_svd_times_x <- function(s, mask, x) {      stopifnot(inherits(mask, \"lgTMatrix\"))      u <- s$u   d <- s$d   v <- s$v      zx <- numeric(nrow(u))      # lgTMatrix uses zero based indexing, add one   row <- mask@i + 1   col <- mask@j + 1      # need to loop over index of indexes   # double looping over i and j here feels intuitive   # but is incorrect   for (idx in seq_along(row)) {     i <- row[idx]     j <- col[idx]          z_ij <- sum(u[i, ] * d * v[j, ])     zx[i] <- zx[i] + x[j] * z_ij   }      zx }  # how to calculate just one element of the reconstructed # data using the SVD  i <- 6 j <- 4  sum(s$u[i, ] * s$d * s$v[j, ]) Z[i, j]  # the whole masked matrix multiply  Z <- s$u %*% diag(s$d) %*% t(s$v) out <- drop((Z * Y) %*% x)  # check that we did this right all.equal(   masked_svd_times_x(s, Y, x),   out ) #include <RcppArmadillo.h>  using namespace arma;  // [[Rcpp::depends(RcppArmadillo)]] // [[Rcpp::export]]  vec masked_svd_times_x_impl(   const mat& U,   const rowvec& d,   const mat& V,   const vec& row,   const vec& col,   const vec& x) {      int i, j;   double z_ij;      vec zx = zeros<vec>(U.n_rows);      for (int idx = 0; idx < row.n_elem; idx++) {          i = row(idx);     j = col(idx);          // % does elementwise multiplication in Armadillo     // accu() gives the sum of elements of resulting vector     z_ij = accu(U.row(i) % d % V.row(j));          zx(i) += x(j) * z_ij;   }      return zx; } # wrap with slightly nicer interface masked_svd_times_x_cpp <- function(s, mask, x) {   drop(masked_svd_times_x_impl(s$u, s$d, s$v, mask@i, mask@j, x)) } bench::mark(   masked_svd_times_x_cpp(s, Y, x),   masked_svd_times_x(s, Y, x) ) #include <RcppArmadillo.h>  using namespace arma;  // [[Rcpp::depends(RcppArmadillo)]] // [[Rcpp::export]]  double p_omega_f_norm_impl(       const mat& U,       const rowvec& d,       const mat& V,       const vec& row,       const vec& col) {        int i, j;     double total = 0;        for (int idx = 0; idx < row.n_elem; idx++) {          i = row(idx);       j = col(idx);          total += accu(U.row(i) % d % V.row(j));     }        return total;   } # wrap with slightly nicer interface p_omega_f_norm_cpp <- function(s, mask) {   p_omega_f_norm_impl(s$u, s$d, s$v, mask@i, mask@j) } all.equal(   sum(Z * Y),   p_omega_f_norm_cpp(s, Y) )"},{"path":"/articles/efficient-sparse-computation.html","id":"a-naive-solution-the-epsilon-trick","dir":"Articles","previous_headings":"check the computation of alpha for p.s.d. matrices > Extension to a mixture of observed and unobserved missingness","what":"A naive solution: the epsilon trick","title":"Using memory-efficient sparse computations","text":"issue: ’ve moved back dense computation land","code":""},{"path":"/articles/efficient-sparse-computation.html","id":"the-memory-efficient-version","dir":"Articles","previous_headings":"check the computation of alpha for p.s.d. matrices > Extension to a mixture of observed and unobserved missingness","what":"The memory efficient version","title":"Using memory-efficient sparse computations","text":"asdf TODO: don’t coerce explicit Matrix class recommended apparently. alternatives?","code":""},{"path":"/articles/efficient-sparse-computation.html","id":"fully-c-implementation-dense","dir":"Articles","previous_headings":"check the computation of alpha for p.s.d. matrices","what":"Fully C++ implementation: Dense","title":"Using memory-efficient sparse computations","text":"","code":"#include <RcppArmadillo.h>  using namespace arma;  // [[Rcpp::depends(RcppArmadillo)]] // [[Rcpp::export]] Rcpp::List AdaptiveInitialize(const sp_mat& M, const int r) {    // coerce to double to avoid integer division   double p_hat = static_cast<double>(M.n_nonzero) / (M.n_cols * M.n_rows);      sp_mat MtM = M.t() * M;   sp_mat MMt = M * M.t();      sp_mat sigma_p = MtM / pow(p_hat, 2) - (1 - p_hat) * diagmat(MtM);   sp_mat sigma_t = MMt / pow(p_hat, 2) - (1 - p_hat) * diagmat(MMt);    mat U_p, V_p, U_t, V_t;   vec s_p, s_t;    svds(U_p, s_p, V_p, sigma_p, r);   svds(U_t, s_t, V_t, sigma_t, r);    // TODO: this is still the eigenvalue calculation   double alpha = (sum(sigma_p.diag()) - sum(s_p)) / (M.n_cols - r);    vec lambda_hat = sqrt(s_p - alpha) / p_hat;    mat U_m, V_m;   vec s_m;    svds(U_m, s_m, V_m, M, r);    // sum(A % B) finds the diag(A^T B) / the diagonal of the cross product   // sum() does a *column-wise sum*, % an elementwise multiplication   rowvec u_sign = sign(sum(U_m % U_t));   rowvec v_sign = sign(sum(V_m % V_p));    rowvec s_hat = u_sign % v_sign;   lambda_hat = lambda_hat % conv_to< vec >::from(s_hat);    return Rcpp::List::create(Rcpp::Named(\"u\") = U_t,                             Rcpp::Named(\"d\") = lambda_hat,                             Rcpp::Named(\"v\") = V_p); } #include <RcppArmadillo.h>  using namespace arma;  // [[Rcpp::depends(RcppArmadillo)]] // [[Rcpp::export]] Rcpp::List AdaptiveImpute(const sp_mat& M, const int r) {      const double EPSILON = 0.000001;            s <- adaptive_initialize(M, r)   Z <- s$u %*% diag(s$d) %*% t(s$v)  # line 1   delta <- Inf      while (delta > epsilon) {          y <- as(M, \"lgCMatrix\")  # indicator if entry of M observed     M_tilde <- M + Z * (1 - y)  # line 3          svd_M <- svds(M_tilde, r)          u_hat <- svd_M$u  # line 4     v_hat <- svd_M$v  # line 5          d <- ncol(M)          alpha <- (sum(M_tilde^2) - sum(svd_M$d^2)) / (d - r)  # line 6          lambda_hat <- sqrt(svd_M$d^2 - alpha)  # line 7          Z_new <- u_hat %*% diag(lambda_hat) %*% t(v_hat)          delta <- sum((Z_new - Z)^2) / sum(Z^2)     Z <- Z_new          print(glue::glue(\"delta: {round(delta, 8)}, alpha: {round(alpha, 3)}\"))   }      Z }"},{"path":"/articles/sparse-matrix-intro.html","id":"abstract","dir":"Articles","previous_headings":"","what":"Abstract","title":"Introduction to sparse computations","text":"TODO: motivate ugh show paper tutorial covering computational details associated computing low-rank rank adaptive imputations matrices described (Cho, Kim, Rohe 2015, 2018). work extends previous data adaptive matrix imputation strategies (Mazumder, Hastie, Tibshirani 2010), better performance eliminating tuning parameters. tutorial proceeds three parts. First, introduce imputation algorithms, useful tidbits linear algebra R package Matrix, use illustrate computations. naive initial implementation eats lots memory, demonstrate memory-efficient implementation. Finally, extend memory-efficient implementation partially observed matrices possibly large number observed zeros. (Bates 2005; Cho, Kim, Rohe 2015, 2018; Maechler Bates 2006; Mazumder, Hastie, Tibshirani 2010; Bro, Acar, Kolda 2007)","code":""},{"path":"/articles/sparse-matrix-intro.html","id":"notation-algorithm","dir":"Articles","previous_headings":"","what":"Notation & Algorithm","title":"Introduction to sparse computations","text":"two steps computing low rank approximation (Cho, Kim, Rohe 2018). First use compute initial low rank estimate AdaptiveInitialize algorithm. step essentially debiased SVD. use initial solution seed AdaptiveImpute algorithm, form iterative SVD thresholding data adaptive thresholding parameter. input algorithms partially observed matrix \\(M\\), \\(r\\), desired rank low-rank approximation. define \\(Y\\) indicator matrix tells us whether observed particular value \\(M\\). \\(\\mathbf{u}_i, \\boldsymbol{\\lambda}_i, \\mathbf{v}_i\\) functions return \\(^{th}\\) left singular vector, singular value, right singular value, respectively. \\(\\tilde \\alpha\\) data adaptive thresholding parameter. Note average uncalculated singular values, thus positive. line 8, \\(\\langle , \\rangle\\) denotes Frobenius inner norm. , vectors \\(x, y\\) matrices \\(, B\\): \\[\\begin{align} \\langle x, y \\rangle &= x^T y = \\sum_{} x_i y_i \\\\ \\langle , B \\rangle &= \\sum_{, j} A_{ij} B_{ij} = \\sum_{ij} \\odot B \\\\ \\end{align}\\] use \\(\\odot B\\) mean elementwise (Hadamard) product matrices \\(\\) \\(B\\). can use output AdaptiveInitialize construct low rank approximation \\(M\\) via: \\[\\begin{align} \\hat M = \\sum_{=1}^r \\hat \\lambda_i \\hat U_i \\hat V_i^T \\end{align}\\] Next AdaptiveImpute: new notation. First define \\[\\begin{align} P_\\Omega() &= \\odot Y \\\\ P_\\Omega^\\perp () &= \\odot (1 - Y) \\end{align}\\] projecttion matrix \\(\\) onto observed \\(M\\) unobserved elements \\(M\\), respectively. Similar \\(\\Omega\\) set pairs \\((, j)\\) \\(M_{, j}\\) observed. \\(\\boldsymbol{\\lambda}_i^2 ()\\) function returns \\(^{th}\\) squared singular value \\(\\) (.e. \\(\\boldsymbol{\\lambda}_i^2 () = \\left(\\boldsymbol{\\lambda}_i ()\\right)^2\\)). Finally \\(\\Vert \\cdot \\Vert_F\\) Frobenius norm matrix.","code":""},{"path":[]},{"path":"/articles/sparse-matrix-intro.html","id":"the-matrix-package","dir":"Articles","previous_headings":"Pre-requisites","what":"The Matrix package","title":"Introduction to sparse computations","text":"haven’t used Matrix package , recommend reading introduction well 2nd introduction. TODO: different storage formats sparse matrices. CSC, triplet, symmetric. part Matrix right thing . triplet form important us later right matrix multiplications hand. repeatedly calculate squared Frobenious norms throughout tutorial. ’s important know many, many ways calculate norm. almost always calculate squared Frobenius norm sparse Matrix M via sum(M@x^2). said, equivalent: projections \\(P_\\Omega ()\\) \\(P_\\Omega^\\perp ()\\) \\(\\Omega\\) indicates observed elements matrix \\(M\\) \\(\\) another matrix dimensions \\(M\\). crossproducts drop() function helps us manage dimensions diagonal crossproduct get eigen() svd(): slightly different stuff: u, d v versus values vectors NOTE: RSpectra truncated decompositions. want full decomposition, use base R stuff. different algos.","code":"library(Matrix)  set.seed(17)  # create a random 8 x 12 sparse matrix with 30 nonzero entries M <- rsparsematrix(8, 12, nnz = 30) M summary(M) # note that Matrix objects are S4 classes so we access their # slots using the @ symbol class(M)  M@x  # vector of values in M M@i  # corresponding row indices  # if you want column indices you need a dgTMatrix M2 <- as(M, \"dgTMatrix\") M2@j  # corresponding column indices // only for dgCMatrix object M^2    # square each element in M elementwise, return as sparse matrix M@x^2  # square each element in M elementwise, return as vector of nonzeros  # the second version is much faster bench::mark(   sum(M@x^2),   sum(M^2),   sum(colSums(M^2)),   norm(M, type = \"F\")^2,   sum(M * M),   iterations = 20 ) y <- as(M, \"lgCMatrix\")  # indicator matrix only all.equal(y * M, M)  # don't lose anything multiplying by indicators  A <- matrix(1:(8*12), 8, 12)  all.equal(dim(A), dim(M))  # appropriate to practice projections with  # Omega indicates whether an entry of M was observed  # P_Omega (A) A * y  !y  # P_Omega^perp (A): NOTE: this results in a *dense* matrix A * (1 - y)  all(A * y + A * (1 - y) == A)  # can recover A from both projections together bench::mark(   crossprod(M),     # dsCMatrix -- most specialized class, want this   crossprod(M, M),  # dgCMatrix   t(M) %*% M,       # dgCMatrix   check = FALSE ) one_col <- matrix(1:4) one_row <- matrix(5:8, nrow = 1)  drop(one_col) drop(one_row)  c(one_col)  # same thing, less explicit. use drop to be explicit v_sign == colSums(svd_M$v * v_hat) diag(t(svd_M$v) %*% v_hat) diag(crossprod(svd_M$v, v_hat))  bench::mark(   colSums(svd_M$v * v_hat),   crossprod(rep(1, d), svd_M$v * v_hat),   iterations = 50,   check = FALSE )  # write a diag_crossprod helper rhos <- matrix(1:12, ncol = 4, byrow = TRUE)  bench::mark(   diag(crossprod(rhos)),   diag(t(rhos) %*% rhos),   colSums(rhos * rhos),   crossprod(rep(1, nrow(rhos)), rhos^2),   check = FALSE                             )   rhos <- matrix(1:12, ncol = 4, byrow = TRUE)  bench::mark(   diag(tcrossprod(rhos)),   diag(crossprod(t(rhos))),   diag(rhos %*% t(rhos)),   rowSums(rhos * rhos),   check = FALSE )"},{"path":"/articles/sparse-matrix-intro.html","id":"brief-aside-in-sign-ambiguity","dir":"Articles","previous_headings":"","what":"Brief aside in sign ambiguity","title":"Introduction to sparse computations","text":"yada yada yada signs left right singular vectors identified SVD elegant solution proposed Bro, Acar, Kolda (2007) used Karl’s paper take inner products NOTE SELF: identifying signs single SVD much harder task comparing two SVDs seeing sign differences. need check sign differences.","code":"set.seed(17) M <- rsparsematrix(8, 12, nnz = 30) # small example, not very sparse  # number of singular vectors to compute k <- 4  s <- svd(M, k, k) s2 <- svds(M, k, k)  # irritating: svd() always gives you all the singular values even if you  # only request the first K singular vectors s$u %*% diag(s$d[1:k]) %*% t(s$v)  # based on the flip_signs function of # https://stats.stackexchange.com/questions/134282/relationship-between-svd-and-pca-how-to-use-svd-to-perform-pca equal_svds <- function(s, s2) {      # svd() always gives you all the singular values, but we only   # want to compare the first k   k <- ncol(s$u)      # the term sign(s$u) * sign(s2$u) performs a sign correction      # isTRUE because output of all.equal is not a boolean, it's something   # weird when the inputs aren't equal. lol why      u_ok <- isTRUE(     all.equal(s$u, s2$u * sign(s$u) * sign(s2$u), check.attributes = FALSE)   )      v_ok <- isTRUE(     all.equal(s$v, s2$v * sign(s$v) * sign(s2$v), check.attributes = FALSE)   )      d_ok <- isTRUE(all.equal(s$d[1:k], s2$d[1:k], check.attributes = FALSE))      u_ok && v_ok && d_ok }"},{"path":"/articles/sparse-matrix-intro.html","id":"linear-algebra-facts","dir":"Articles","previous_headings":"Brief aside in sign ambiguity","what":"Linear algebra facts","title":"Introduction to sparse computations","text":"Throughout computations, repeatedly use several key facts eigendecompositions, singular value decompositions (SVD) relationship two. https://en.wikipedia.org/wiki/Gramian_matrix X’X – positive semi-def, singular values M’M eigenvalues question: , B positive, sum(svd(- B)\\(d) == sum(svd()\\)d) - sum(svd(B)$d) answer: ! can’t split two easy computations combine possibly use get sort bound? think happens p_hat -> 0 key observation \\(M^T M\\) Fact: sum squared singular values trace(^T ) https://math.stackexchange.com/questions/2281721/sum--singular-values---matrix Fact: symmetric positive definite matrices eigendecomp equal singular value decomp Fact: sum eigenvalues M equal trace(M) Consequence: pos def symmetric M sum singular values trace(M) well computing alpha deserves explanation. reference: https://math.stackexchange.com/questions/1463269/--obtain-sum--square--eigenvalues-without-finding-eigenvalues Frobenius norm () = trace(crossprod()) TODO: know thing strictly positive prevent sqrt() exploding FACT: sum(diag(crossprod(M))) == sum(M^2)","code":"## STOPPED HERE: WHY ARE the following not the same?   isSymmetric(sigma_p)   eigen(sigma_p)$values   sum(diag(sigma_p))   sum(svd(sigma_p)$d)   sum(eigen(sigma_p)$values)      # let's think just about the first term for a moment   sum(diag(MtM / p_hat^2))   sum(svd(MtM / p_hat^2)$d)      sum(colSums(M^2 / p_hat^2))      # has some negative eigenvalues      # Fact: for a symmetric matrix, the singular values are the *absolute* values   # of the eigenvalues      # https://www.mathworks.com/content/dam/mathworks/mathworks-dot-com/moler/eigs.pdf      eigen(sigma_p)$values   sum(eigen(sigma_p)$values)   sum(abs(eigen(sigma_p)$values))   sum(svd(sigma_p)$d)   sum(abs(diag(sigma_p)))      # those agree so what about the second term      # note to self: alpha should be positive   # issue karl ran into:   # https://math.stackexchange.com/questions/381808/sum-of-eigenvalues-and-singular-values   # how to get the sum of singular values itself (start):   # https://math.stackexchange.com/questions/569989/sum-of-singular-values-of-ab      # options when sigma_p is not positive definite:   # - calculate the full SVD   # - set alpha to zero (don't truncate the singular values)   # - this lower bounds the average of the remaining singular values   # -      # positive semi-definite is enough since symmetric and eigen/singular values   # of zero don't matter      # this is only an issue in the initialization. in the iterative updates   # we use the squared singular values, which we can more easily calculate   # the sum of      # ask Karl what he wants to do about this: computing a full SVD is gonna be really expensive."},{"path":"/articles/sparse-matrix-intro.html","id":"reference-implementation","dir":"Articles","previous_headings":"","what":"Reference implementation","title":"Introduction to sparse computations","text":"’s worth commenting computation alpha s_hat. compute alpha line 20 adaptive_initialize(), don’t want full eigendecomposition \\(\\Sigma_{\\hat p}\\) since take long time, use trick recall trace matrix (sum ’s diagonal elements) equals sum eigenvalues. subtract first \\(r\\) eigenvalues, compute, left \\(\\sum_{= r + 1}^d \\lambda_i(\\Sigma_{\\hat p})\\). Finally can minimal sanity check see code even runs, see recovering something close-ish implanted low-rank structure.","code":"library(RSpectra) library(Matrix) adaptive_initialize <- function(M, r) {      # TODO: ignores observed zeros!   p_hat <- nnzero(M) / prod(dim(M))  # line 1      MtM <- crossprod(M)   MMt <- tcrossprod(M)      # need to divide by p^2 from Cho et al 2016 to get the \"right\"   # singular values / singular values on a comparable scale      # both of these matrices are symmetric, but not necessarily positive   # this has important implications for the SVD / eigendecomp relationship      sigma_p <- MtM / p_hat^2 - (1 - p_hat) * diag(diag(MtM))  # line 2   sigma_t <- MMt / p_hat^2 - (1 - p_hat) * diag(diag(MMt))  # line 3      # crossprod() and tcrossprod() return dsCMatrix objects,   # sparse matrix objects that know they are symmetric      # unfortunately, RSpectra doesn't support dsCMatrix objects,   # but does support dgCMatrix objects, a class representing sparse   # but not symmetric matrices      # support for dsCMatrix objects in RSpectra is on the way,   # which will eliminate the need for the following coercions.   # see: https://github.com/yixuan/RSpectra/issues/15      sigma_p <- as(sigma_p, \"dgCMatrix\")   sigma_t <- as(sigma_t, \"dgCMatrix\")      svd_p <- svds(sigma_p, r)  # TODO: is eigs_sym() faster?   svd_t <- svds(sigma_t, r)      v_hat <- svd_p$v  # line 4   u_hat <- svd_t$u  # line 5      n <- nrow(M)   d <- ncol(M)      # NOTE: alpha is incorrect due to singular values and eigenvalues   # being different when sigma_p is not positive      alpha <- (sum(diag(sigma_p)) - sum(svd_p$d)) / (d - r)  # line 6   lambda_hat <- sqrt(svd_p$d - alpha) / p_hat             # line 7      svd_M <- svds(M, r)      v_sign <- crossprod(rep(1, d), svd_M$v * v_hat)   u_sign <- crossprod(rep(1, n), svd_M$u * u_hat)   s_hat <- drop(sign(v_sign * u_sign))      lambda_hat <- lambda_hat * s_hat  # line 8      list(u = u_hat, d = lambda_hat, v = v_hat) } adaptive_impute <- function(M, r, epsilon = 1e-7) {      s <- adaptive_initialize(M, r)   Z <- s$u %*% diag(s$d) %*% t(s$v)  # line 1   delta <- Inf      while (delta > epsilon) {          y <- as(M, \"lgCMatrix\")  # indicator if entry of M observed     M_tilde <- M + Z * (1 - y)  # line 3          svd_M <- svds(M_tilde, r)          u_hat <- svd_M$u  # line 4     v_hat <- svd_M$v  # line 5          d <- ncol(M)          alpha <- (sum(M_tilde^2) - sum(svd_M$d^2)) / (d - r)  # line 6          lambda_hat <- sqrt(svd_M$d^2 - alpha)  # line 7          Z_new <- u_hat %*% diag(lambda_hat) %*% t(v_hat)          delta <- sum((Z_new - Z)^2) / sum(Z^2)     Z <- Z_new          print(glue::glue(\"delta: {round(delta, 8)}, alpha: {round(alpha, 3)}\"))   }      Z } n <- 500 d <- 100 r <- 5  A <- matrix(runif(n * r, -5, 5), n, r) B <- matrix(runif(d * r, -5, 5), d, r) M0 <- A %*% t(B)  err <- matrix(rnorm(n * d), n, d) Mf <- M0 + err  p <- 0.3 y <- matrix(rbinom(n * d, 1, p), n, d) dat <- Mf * y  init <- adaptive_initialize(dat, r) filled <- adaptive_impute(dat, r)"},{"path":"/articles/sparse-matrix-intro.html","id":"references","dir":"Articles","previous_headings":"","what":"References","title":"Introduction to sparse computations","text":"Bates, Douglas. 2005. “Introduction Matrix Package.” https://cran.r-project.org/web/packages/Matrix/vignettes/Introduction.pdf. Bro, Rasmus, Evrim Acar, Tamara G. Kolda. 2007. “Resolving Sign Ambiguity Singular Value Decomposition,” 18. Cho, Juhee, Donggyu Kim, Karl Rohe. 2015. “Asymptotic Theory Estimating Singular Vectors Values Partially-Observed Low Rank Matrix Noise.” arXiv:1508.05431 [Stat], August. http://arxiv.org/abs/1508.05431. ———. 2018. “Intelligent Initialization Adaptive Thresholding Iterative Matrix Completion; Statistical Algorithmic Theory Adaptive-Impute.” Journal Computational Graphical Statistics, September, 1–26. https://doi.org/10.1080/10618600.2018.1518238. Maechler, Martin, Douglas Bates. 2006. “2nd Introduction Matrix Package.” https://cran.r-project.org/web/packages/Matrix/vignettes/Intro2Matrix.pdf. Mazumder, Rahul, Trevor Hastie, Robert Tibshirani. 2010. “Spectral Regularization Algorithms Learning Large Incomplete Matrices.” https://web.stanford.edu/~hastie/Papers/mazumder10a.pdf.","code":""},{"path":"/authors.html","id":null,"dir":"","previous_headings":"","what":"Authors","title":"Authors and Citation","text":"Alex Hayes. Author, maintainer, copyright holder. Juhee Cho. Author. Donggyu Kim. Author. Karl Rohe. Author.","code":""},{"path":"/authors.html","id":"citation","dir":"","previous_headings":"","what":"Citation","title":"Authors and Citation","text":"Hayes , Cho J, Kim D, Rohe K (2022). fastadi: Self-Tuning Data Adaptive Matrix Imputation. R package version 0.0.0.9019, https://github.com/RoheLab/fastadi.","code":"@Manual{,   title = {fastadi: Self-Tuning Data Adaptive Matrix Imputation},   author = {Alex Hayes and Juhee Cho and Donggyu Kim and Karl Rohe},   year = {2022},   note = {R package version 0.0.0.9019},   url = {https://github.com/RoheLab/fastadi}, }"},{"path":"/index.html","id":"fastadi","dir":"","previous_headings":"","what":"Self-Tuning Data Adaptive Matrix Imputation","title":"Self-Tuning Data Adaptive Matrix Imputation","text":"fastadi implements AdaptiveImpute matrix completion algorithm. fastadi self-tuning alternative algorithms SoftImpute (implemented softImpute package), truncated SVD, maximum margin matrix factorization, weighted regularized matrix factorization (implemented rsparse package). simulations fastadi often outperforms softImpute small margin. may find fastadi useful developing embeddings sparsely observed data, working natural language processing, building recommendation system.","code":""},{"path":"/index.html","id":"installation","dir":"","previous_headings":"","what":"Installation","title":"Self-Tuning Data Adaptive Matrix Imputation","text":"can install released version CRAN : can install development version GitHub :","code":"install.packages(\"fastadi\") # install.packages(\"devtools\") devtools::install_github(\"RoheLab/fastadi\")"},{"path":"/index.html","id":"example-usage","dir":"","previous_headings":"","what":"Example usage","title":"Self-Tuning Data Adaptive Matrix Imputation","text":"embed users items MovieLens 100K dataset. Note vignettes currently scratch work reference developers yet ready general consumption.","code":"library(fastadi) #> Loading required package: LRMF3 #> Loading required package: Matrix mf <- adaptive_impute(ml100k, rank = 3L, max_iter = 5L) #> Warning:  #> Reached maximum allowed iterations. Returning early. mf #>  #> Adaptively Imputed Low Rank Matrix Factorization #> ------------------------------------------------ #>  #> Rank: 3 #>  #> Rows: 943 #> Cols: 1682 #>  #> d[rank]: 467.486 #> alpha:   144.663 #>  #> Components #>  #> u: 943 x 3 [matrix]  #> d: 3      [numeric]  #> v: 1682 x 3 [matrix]"},{"path":"/index.html","id":"references","dir":"","previous_headings":"","what":"References","title":"Self-Tuning Data Adaptive Matrix Imputation","text":"Cho, Juhee, Donggyu Kim, Karl Rohe. “Asymptotic Theory Estimating Singular Vectors Values Partially-Observed Low Rank Matrix Noise.” Statistica Sinica, 2018. https://doi.org/10.5705/ss.202016.0205. ———. “Intelligent Initialization Adaptive Thresholding Iterative Matrix Completion: Statistical Algorithmic Theory Adaptive-Impute.” Journal Computational Graphical Statistics 28, . 2 (April 3, 2019): 323–33. https://doi.org/10.1080/10618600.2018.1518238. can find original implementation accompanying papers .","code":""},{"path":"/reference/adaptive_imputation.html","id":null,"dir":"Reference","previous_headings":"","what":"Create an Adaptive Imputation object — adaptive_imputation","title":"Create an Adaptive Imputation object — adaptive_imputation","text":"adaptive_imputation objects subclass LRMF3::svd_like(), additional field alpha.","code":""},{"path":"/reference/adaptive_imputation.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Create an Adaptive Imputation object — adaptive_imputation","text":"","code":"adaptive_imputation(u, d, v, alpha, ...)"},{"path":"/reference/adaptive_imputation.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Create an Adaptive Imputation object — adaptive_imputation","text":"u matrix \"left singular-ish\" vectors. d vector \"singular-ish\" values. v matrix \"right singular-ish\" vectors. alpha Value alpha final iteration. ... Optional additional items pass constructor.","code":""},{"path":"/reference/adaptive_imputation.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Create an Adaptive Imputation object — adaptive_imputation","text":"adaptive_imputation object.","code":""},{"path":"/reference/adaptive_impute.html","id":null,"dir":"Reference","previous_headings":"","what":"AdaptiveImpute — adaptive_impute","title":"AdaptiveImpute — adaptive_impute","text":"implementation AdaptiveImpute algorithm matrix completion sparse matrices.","code":""},{"path":"/reference/adaptive_impute.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"AdaptiveImpute — adaptive_impute","text":"","code":"adaptive_impute(   X,   rank,   ...,   initialization = c(\"svd\", \"adaptive-initialize\", \"approximate\"),   max_iter = 200L,   check_interval = 1L,   epsilon = 1e-07,   additional = NULL )  # S3 method for sparseMatrix adaptive_impute(   X,   rank,   ...,   initialization = c(\"svd\", \"adaptive-initialize\", \"approximate\"),   additional = NULL )  # S3 method for LRMF adaptive_impute(   X,   rank,   ...,   epsilon = 1e-07,   max_iter = 200L,   check_interval = 1L )"},{"path":"/reference/adaptive_impute.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"AdaptiveImpute — adaptive_impute","text":"X sparse matrix Matrix::sparseMatrix() class. rank Desired rank (integer) use low rank approximation. Must least 2L rank X. Note rank X typically unobserved computations may unstable even fail rank near exceeds threshold. ... Unused additional arguments. initialization initialize low rank approximation. Options : \"svd\" (default). initialization step, treats unobserved values zeroes. \"adaptive-initialize\". initialization step, treats unobserved values actually unobserved. However, current AdaptiveInitialize implementation relies dense matrix computations suitable relatively small matrices. \"approximate\". approximate variant AdaptiveInitialize less computationally expensive. See adaptive_initialize details. Note initialization matters AdaptiveImpute optimizes non-convex objective. current theory shows initializing AdaptiveInitialize leads consistent estimator, know case SVD initialization. Empirically found SVD initialization works well nonetheless. max_iter Maximum number iterations perform (integer). Defaults 200L. practice 10 iterations get decent approximation use exploratory analysis, 50-100 get way convergence. Must least 1L. check_interval Integer specifying often perform convergence checks. Defaults 1L. practice, check convergence requires norm calculation expensive large matrices decreasing frequency convergence checks reduce computation time. Can also set NULL, case max_iter iterations algorithm occur possibility stopping due small relative change imputed matrix. case delta reported Inf. epsilon Convergence criteria, measured terms relative change Frobenius norm full imputed matrix. Defaults 1e-7. additional Ignored except alpha_method = \"approximate\" case controls precise approximation alpha. approximate computation alpha always understand alpha, approximation better larger values additional. recommend making additional large computationally tolerable.","code":""},{"path":"/reference/adaptive_impute.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"AdaptiveImpute — adaptive_impute","text":"low rank matrix factorization represented adaptive_imputation() object.","code":""},{"path":"/reference/adaptive_impute.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"AdaptiveImpute — adaptive_impute","text":"Cho, Juhee, Donggyu Kim, Karl Rohe. “Asymptotic Theory Estimating Singular Vectors Values Partially-Observed Low Rank Matrix Noise.” Statistica Sinica, 2018. https://doi.org/10.5705/ss.202016.0205. ———. “Intelligent Initialization Adaptive Thresholding Iterative Matrix Completion: Statistical Algorithmic Theory Adaptive-Impute.” Journal Computational Graphical Statistics 28, . 2 (April 3, 2019): 323–33. https://doi.org/10.1080/10618600.2018.1518238.","code":""},{"path":"/reference/adaptive_impute.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"AdaptiveImpute — adaptive_impute","text":"","code":"### SVD initialization (default) --------------------------------------------  mf <- adaptive_impute(ml100k, rank = 50L, max_iter = 20L) #> Warning:  #> Reached maximum allowed iterations. Returning early.  ### Exact AdaptiveInitialize initialization ---------------------------------  mf2 <- adaptive_impute(   ml100k,   rank = 3L,   max_iter = 20L,   initialization = \"adaptive-initialize\" ) #> Warning:  #> Reached maximum allowed iterations. Returning early.  ### Approximate AdaptiveInitialize initialization ---------------------------  mf3 <- adaptive_impute(   ml100k,   rank = 3L,   max_iter = 20L,   initialization = \"approximate\",   additional = 25 ) #> Warning:  #> Reached maximum allowed iterations. Returning early."},{"path":"/reference/adaptive_initialize.html","id":null,"dir":"Reference","previous_headings":"","what":"AdaptiveInitialize — adaptive_initialize","title":"AdaptiveInitialize — adaptive_initialize","text":"implementation AdaptiveInitialize algorithm matrix imputation sparse matrices. moment implementation suitable small matrices order thousands rows columns .","code":""},{"path":"/reference/adaptive_initialize.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"AdaptiveInitialize — adaptive_initialize","text":"","code":"adaptive_initialize(   X,   rank,   ...,   p_hat = NULL,   alpha_method = c(\"exact\", \"approximate\"),   additional = NULL )  # S3 method for sparseMatrix adaptive_initialize(   X,   rank,   ...,   p_hat = NULL,   alpha_method = c(\"exact\", \"approximate\"),   additional = NULL )"},{"path":"/reference/adaptive_initialize.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"AdaptiveInitialize — adaptive_initialize","text":"X sparse matrix sparseMatrix class. Explicit (observed) zeroes X can dropped rank Desired rank (integer) use low rank approximation. Must least 2L rank X. ... Ignored. p_hat portion X observed. Defaults NULL, case p_hat set number observed elements X. Primarily internal use citation_impute() advanced users. alpha_method Either \"exact\" \"approximate\", defaulting \"exact\". \"exact\" computationally expensive requires taking complete SVD matrix size nrow(X) x nrow(X), matches AdaptiveInitialize algorithm exactly. \"approximate\" departs AdaptiveInitialization algorithm compute truncated SVD rank rank + additional instead complete SVD. reduces computational burden, resulting estimates singular-ish values penalized much AdaptiveInitialize algorithm. additional Ignored except alpha_method = \"approximate\" case controls precise approximation alpha. approximate computation alpha always understand alpha, approximation better larger values additional. recommend making additional large computationally tolerable.","code":""},{"path":"/reference/adaptive_initialize.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"AdaptiveInitialize — adaptive_initialize","text":"low rank matrix factorization represented adaptive_imputation() object.","code":""},{"path":"/reference/adaptive_initialize.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"AdaptiveInitialize — adaptive_initialize","text":"","code":"mf <- adaptive_initialize(ml100k, rank = 3L) mf #>  #> Adaptively Imputed Low Rank Matrix Factorization #> ------------------------------------------------ #>  #> Rank: 3 #>  #> Rows: 943 #> Cols: 1682 #>  #> d[rank]: 3334.419 #> alpha:   684.448 #>  #> Components #>  #> u: 943 x 3 [matrix]  #> d: 3      [numeric]  #> v: 1682 x 3 [matrix]"},{"path":"/reference/citation_impute.html","id":null,"dir":"Reference","previous_headings":"","what":"CitationImpute — citation_impute","title":"CitationImpute — citation_impute","text":"implementation AdaptiveImpute algorithm using efficient sparse matrix computations, specialized case missing values upper triangle taken explicitly observed zeros, opposed missing values. primarily useful spectral decompositions adjacency matrices graphs (near) tree structure, citation networks.","code":""},{"path":"/reference/citation_impute.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"CitationImpute — citation_impute","text":"","code":"citation_impute(   X,   rank,   ...,   initialization = c(\"svd\", \"adaptive-initialize\", \"approximate\"),   max_iter = 200L,   check_interval = 1L,   epsilon = 1e-07,   additional = NULL )  # S3 method for sparseMatrix citation_impute(   X,   rank,   ...,   initialization = c(\"svd\", \"adaptive-initialize\", \"approximate\"),   additional = NULL )  # S3 method for LRMF citation_impute(   X,   rank,   ...,   epsilon = 1e-07,   max_iter = 200L,   check_interval = 1L )"},{"path":"/reference/citation_impute.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"CitationImpute — citation_impute","text":"X square sparse matrix Matrix::sparseMatrix() class. Implicit zeros upper triangle matrix considered observed predictions elements contribute objective function minimized AdaptiveImpute. rank Desired rank (integer) use low rank approximation. Must least 2L rank X. Note rank X typically unobserved computations may unstable even fail rank near exceeds threshold. ... Unused additional arguments. initialization initialize low rank approximation. Options : \"svd\" (default). initialization step, treats unobserved values zeroes. \"adaptive-initialize\". initialization step, treats unobserved values actually unobserved. However, current AdaptiveInitialize implementation relies dense matrix computations suitable relatively small matrices. \"approximate\". approximate variant AdaptiveInitialize less computationally expensive. See adaptive_initialize details. Note initialization matters AdaptiveImpute optimizes non-convex objective. current theory shows initializing AdaptiveInitialize leads consistent estimator, know case SVD initialization. Empirically found SVD initialization works well nonetheless. max_iter Maximum number iterations perform (integer). Defaults 200L. practice 10 iterations get decent approximation use exploratory analysis, 50-100 get way convergence. Must least 1L. check_interval Integer specifying often perform convergence checks. Defaults 1L. practice, check convergence requires norm calculation expensive large matrices decreasing frequency convergence checks reduce computation time. Can also set NULL, case max_iter iterations algorithm occur possibility stopping due small relative change imputed matrix. case delta reported Inf. epsilon Convergence criteria, measured terms relative change Frobenius norm full imputed matrix. Defaults 1e-7. additional Ignored except alpha_method = \"approximate\" case controls precise approximation alpha. approximate computation alpha always understand alpha, approximation better larger values additional. recommend making additional large computationally tolerable.","code":""},{"path":"/reference/citation_impute.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"CitationImpute — citation_impute","text":"low rank matrix factorization represented adaptive_imputation() object.","code":""},{"path":"/reference/citation_impute.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"CitationImpute — citation_impute","text":"","code":"# create a (binary) square sparse matrix to demonstrate on  set.seed(887)  n <- 100 A <- rsparsematrix(n, n, 0.1, rand.x = NULL)  ### SVD initialization (default) --------------------------------------------  mf <- citation_impute(A, rank = 3L, max_iter = 10L) #> Warning:  #> Reached maximum allowed iterations. Returning early.  ### Exact AdaptiveInitialize initialization ---------------------------------  mf2 <- citation_impute(   A,   rank = 3L,   max_iter = 10L,   initialization = \"adaptive-initialize\" ) #> Warning:  #> Reached maximum allowed iterations. Returning early.  ### Approximate AdaptiveInitialize initialization ---------------------------  mf3 <- citation_impute(   A,   rank = 3L,   max_iter = 10L,   initialization = \"approximate\",   additional = 5L ) #> Warning:  #> Reached maximum allowed iterations. Returning early."},{"path":"/reference/fastadi-package.html","id":null,"dir":"Reference","previous_headings":"","what":"fastadi: Self-Tuning Data Adaptive Matrix Imputation — fastadi-package","title":"fastadi: Self-Tuning Data Adaptive Matrix Imputation — fastadi-package","text":"Implements AdaptiveImpute matrix completion algorithm 'Intelligent Initialization Adaptive Thresholding Iterative Matrix Completion', <https://amstat.tandfonline.com/doi/abs/10.1080/10618600.2018.1518238>. AdaptiveImpute useful embedding sparsely observed matrices, often performs competing matrix completion algorithms, self-tunes hyperparameter, making usage easy.","code":""},{"path":[]},{"path":"/reference/fastadi-package.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"fastadi: Self-Tuning Data Adaptive Matrix Imputation — fastadi-package","text":"Maintainer: Alex Hayes alexpghayes@gmail.com (ORCID) [copyright holder] Authors: Juhee Cho Donggyu Kim Karl Rohe","code":""},{"path":"/reference/masked_approximation_impl.html","id":null,"dir":"Reference","previous_headings":"","what":"Expand an SVD only at observed values of a sparse matrix — masked_approximation_impl","title":"Expand an SVD only at observed values of a sparse matrix — masked_approximation_impl","text":"TODO: describe looks like dimensions match s mask. See vignette(\"sparse-computations\") mathematical details.","code":""},{"path":"/reference/masked_approximation_impl.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Expand an SVD only at observed values of a sparse matrix — masked_approximation_impl","text":"","code":"masked_approximation_impl(U, V, row, col)"},{"path":"/reference/masked_approximation_impl.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Expand an SVD only at observed values of a sparse matrix — masked_approximation_impl","text":"U Low-rank matrix left singular-ish vectors. V Low-rank matrix right singular-ish vectors. row Zero-based row indices observed elements. col Zero-based col indices observed elements.","code":""},{"path":"/reference/masked_approximation_impl.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Expand an SVD only at observed values of a sparse matrix — masked_approximation_impl","text":"sparse matrix representing low-rank reconstruction U, d V, index pairs indicated row col.","code":""},{"path":"/reference/masked_approximation_impl.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Expand an SVD only at observed values of a sparse matrix — masked_approximation_impl","text":"idea populate U, d V using elements SVD-like list. can generate row col easily sparse masking Matrix (Matrix package), coercing triplet format, extracting mask@row mask@j column.","code":""}]
